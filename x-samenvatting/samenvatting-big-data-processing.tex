% play phantom dust

\documentclass[a4paper,10pt,twoside]{report}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{titling}
\usepackage{fontspec}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}

\usepackage[dvipsnames]{xcolor}

\lstdefinelanguage{Java}{
	keywords=[1]{
		fields, schemaData, dataframe, latMin, latMax, longMin, longMax, rightOuter, leftOuter, lat1, lon1, lat2, lon2, deltaLat, deltaLon, c, d, a, radius, distance, time, tokenizer, remover, assembler, linreg, rfr, trainsetsplit, traintestsplit, indexer, minmax, pipelinerfr, paramgridrfr, pipelinelinreg, trainedlinreg, model, cvmrfr, evaluator, calc, log, metrictype, metrictypes, svc, pca, paramgridsvm, cvsvm, predictionssvm, metrics, predictionsLogReg, predictions, train, test, trainsetsplit, traintestsplit, datasets, createsubsample, pipelinesvm, glr, paramgridglr, gbt, paramgridgbt, pipelineglr, pipelinegbt, predictionsrfr, cvrfr, trainvalidationsplitglr, trainvalidationsplitgbt, range, nietfraudulent, lengte, arrfeatures, kolom, creditcardsubsample, cvmSVM, trainingset, testset, matrix, r1, vectorizer, lr, paramgridlogreg, pipelinelogreg, htf, rfc, paramgridrfc, cvrfc, pipelinerfc, pipelinemodellogreg, predictedlogreg, predictionsrfc},
	keywordstyle=[1]\color{Bittersweet},
	keywords=[2]{linearsvc, pipeline, parammap, minmaxscaler, vectorassembler, crossvalidatormodel, linearregression, pipelinemodel, stringindexer, randomforestregressor, crossvalidator, regressionevaluator, regextokenizer, stopwordsremover, countvectorizer, hashingtf, logisticregression, paramgridbuilder, randomforestclassifier}, % ML-typen
	keywordstyle=[2]\color{purple},	
	keywords=[3]{private, static, final, throws, public, exception, call, udf4, udf2, new, return, system, printf, try, finally, catch}, 
	keywordstyle=[3]\color{violet},
	keywords=[4]{double, doubletype, timestamptype, stringtype, integertype, boolean , int, long, string, dataset},
	keywordstyle=[4]\color{RoyalBlue},
	keywords=[5]{label, prediction, verhouding, metric, spark},
	keywordstyle=[5]\color{Aquamarine}\bfseries,
	keywords=[6]{getTraining, getTest, clean, haversine, speed, printcorrelation, getarearoccurve, printconfusionmatrixmetrics, printregressionevaluation, getrangedataframe, getdata, createsubmission, adddistance, addspeed},
	keywordstyle=[6]\color{OliveGreen}\bfseries,
	keywords=[7]{where, select, first, orderby, groupby, count, show, drop, mean, sum, fit, transform, randomsplit, getdouble, withcolumn, union, sample, columns, weightedprecision, weightedrecall, accuracy},
	keywordstyle=[7]\color{PineGreen},
	keywords=[8]{pickuplatitude, pickuplongitude, dropofflatitude, dropofflongitude}, %input
	keywordstyle=[8]\color{Periwinkle},
	identifierstyle=\color{black},
	sensitive=false,
	comment=[l]{//},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{red}\ttfamily,
	stringstyle=\color{Sepia}\ttfamily,
	morestring=[b]',
	morestring=[b]"
}

\lstset{ %
	backgroundcolor=\color{white},   
	basicstyle=\footnotesize,        
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	commentstyle=\color{commentsColor}\textit,
	deletekeywords={...},            % if you want to delete keywords from the given language
	escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
	extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	frame=tb,	                   	   % adds a frame around the code
	keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	keywordstyle=\color{keywordsColor},       % keyword style
	language=Python,                 % the language of the code (can be overrided per snippet)
	otherkeywords={*,...},           % if you want to add more keywords to the set
	numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
	numbersep=5pt,                   % how far the line-numbers are from the code
	numberstyle=\tiny\color{commentsColor}, % the style that is used for the line-numbers
	rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
	showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
	showstringspaces=false,          % underline spaces within strings only
	showtabs=false,                  % show tabs within strings adding particular underscores
	stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
	stringstyle=\color{stringColor}, % string literal style
	tabsize=2,	                   % sets default tabsize to 2 spaces
	title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
	columns=fixed                    % Using fixed column width (for e.g. nice alignment)
}

\newfontfamily\headingfont[]{Montserrat-Black}

\usepackage[dutch]{babel}

\usepackage{fontspec}
\setmainfont{Montserrat}

\title{Samenvatting Big Data Processing}

\author{Dylan {Cluyse}}

\begin{document}
	
\maketitle
	
\tableofcontents
	
\chapter{Big Data Processing}

\section{Inleiding}

\subsection{Breed beeld}

Een gedistribueerd bestandssysteem dient om data, verspreid over meerdere computers, op te slaan en te beheren binnenin een cluster. In tegenstelling tot een traditioneel bestandssysteem, waar de gegevens op één centrale server worden opgeslaan, worden de gegevens in een gedistribueerd bestandssysteem op verschillende computers in een cluster opgeslaan. 

\begin{itemize}
	\item Deze technologie biedt zowel concurrency als redundantie aan. Het is nu mogelijk om riante hoeveelheden gegevens op te slaan en te verwerken. Het systeem wordt niet meer beperkt door de beperkte opslagruimte van één server.
	\item Het kan ook helpen om de prestaties te verbeteren door het gebruik van parallellisatie, waardoor meerdere computers tegelijkertijd kunnen werken aan het verwerken van de gegevens.
	\item Verschillende componenten op een netwerk die met elkaar communiceren.
	\item Een systeem met het doel om data beschikbaar te maken. Data dat later kan worden gelezen of geschreven.
	\item De mate van beschikbaarheid doet er niet toe.
\end{itemize}

\subsection{De nood aan schaalbaarheid}

In een gedistribueerd bestandssysteem is er een verschil tussen horizontale en verticale schaalbaarheid.

\subsubsection{Horizontale schaalbaarheid}

Horizontale schaalbaarheid betekent dat het systeem kan worden uitgebreid door het toevoegen van meer computers aan het cluster, wat kan helpen om meer gegevens te verwerken en om de prestaties te verbeteren. 

\subsubsection{Verticale schaalbaarheid}

Verticale schaalbaarheid betekent dat het systeem kan worden uitgebreid door het toevoegen van meer hardware aan een enkele computer, zoals extra geheugen of een snellere processor. Dit kan ook helpen om de prestaties te verbeteren, maar het is beperkter dan horizontale schaalbaarheid omdat het alleen mogelijk is binnen de beperkingen van een enkele computer.

\subsubsection{Moore's Law}

Moore's Law geeft aan dat het aantal transistoren per achttien maanden verdubbelt. Er is een snelle nood aan nieuwe hardwarematerialen. Daarnaast is er ook nood aan nieuwe hardwarematerialen en fouttolerantie in een databankomgeving. 

\subsubsection{Region-picking}

Het verkeer bij een databankserver gebeurt online, wat betekent dat de latency een rol speelt voor de gebruiker. De client moet een systeem kiezen die zo dicht mogelijk bij de client ligt. Online gameservers zijn een goede casus waarbij de systemen opgedeeld zijn per regio.

\subsection{Kenmerken}

Een gedistribueerd systeem omvat drie kenmerken:

\begin{itemize}
	\item Er is geen gedeeld geheugen. Iedere verwerkingseenheid heeft een eigen geheugen.
	\item Componenten zijn niet bewust van wat de andere componenten nu aan het doen zijn. Daarom sturen ze onderling berichten naar elkaar.
	\item Fouttolerant
\end{itemize}

\section{Soorten systemen}

Er zijn twee soorten systemen: parallele en gedistribueerde systemen.

\begin{itemize}
	\item Bij een parallel systeem worden meerdere processen tegelijkertijd uitgevoerd. Deze verwerking gebeurt op verschillende verwerkingseenheden met een gedeeld geheugen. Het is makkelijker te ontwikkelen, maar met de kost van géén redundantie.
	\item Een gedistribueerd systeem bevat verschillende verwerkingseenheden met elk een eigen geheugen. De andere componenten zijn onbewust van wat de andere onderdelen aan het doen zijn. Er wordt onderling berichten met elkaar verstuurd. Dit noemt ook een shared-nothing architecture: Er wordt niets onderling gedeeld. De enige manier van communicatie is door middel van boodschappen.
\end{itemize}

\subsection{CAP-theorem}

Deze theorie wijst aan dat er in een gedistribueerd databanksysteem een afweging moet worden gemaakt tussen drie factoren. Het is onmogelijk om alle drie deze eigenschappen tegelijkertijd te garanderen. Er moet een afweging worden gemaakt. Deze stelling komt terug uit databanken, maar hier is het anders. De consistentie van een relationele databank overstijgt de consistentie van een gedistribueerde databank.

\begin{itemize}
	\item Consistentie wijst aaan dat alle nodes dezelfde gegevens hebben.
	\item Beschikbaarheid wijst aan dat het systeem beschikbaar blijft voor alle gebruikers, zelfs al is er een crash of een paar nodes die niet beschikbaar zijn.
	\item Partitioneringstolerantie wijst aan dat het systeem blijft functioneren als er problemen zijn met de communicatie tussen de nodes.
\end{itemize}



\subsection{Struikelblokken}

Bij het werken met gedistribueerde systemen zijn er vier verschillende struikelblokken waarmee er moet worden rekening gehouden:

\begin{itemize}
	\item Split-brain scenario: de ene helft denkt het ene en de andere helft denkt iets anders. Bijvoorbeeld: Het ene systeem denkt dat een bestand verwijderd is, terwijl het andere systeem denkt dat het nog bestaat.
	\item Consistency en structuur raken snel verloren.
	\item Testen wordt moeilijker.
	\item De oorzaak van traagheid achterhalen wordt complexer: zowel hardware als software kunnen een rol spelen.
\end{itemize}


\subsection{Fabels over gedistribueerde systemen}


\begin{itemize}
	\item "Er is geen latency". Latency is wel aanwezig. De latency vermindert afhankelijk van twee punten: de locatie van het systeem en de locatie van de gebruiker. Het duurt een tijd vooraleer een bericht op een systeem aankomt. Er is ook een verschil tussen de positie van een systeem op een \textit{data rack}.
	\item "De bandbreedte is oneindig". De bandbreedte op zowel de client als het distribueerd systeem is beperkt.
	\item "Het netwerk is veilig". Toegang tot het netwerk is belangrijk.
	\item "De netwerktopologie blijft hetzelfde". Computers en hardware kan worden toegevoegd. Zo verandert alles binnen een netwerk op een dynamische manier.
	\item "De transportkost van data is nul". Data transporteren van begin- naar eindpunt vergt een inspanning qua energie en rekenkracht.
	\item "Het netwerk is homogeen". Alle onderdelen binnen een netwerk kunnen variëren van eigenschappen. Sommige delen van het netwerk kunnen snel zijn, sommige delen zijn traag.
\end{itemize}

\section{Problemen met gedistribueerde netwerken}

Bij een gedistribueerd systeem zijn er vier algemene problemen:

\begin{itemize}
	\item Partial failures
	\item Niet-betrouwbare netwerken
	\item Niet-betrouwbare tijdsindicaties
	\item Onderlinge onzekerheid
\end{itemize}

\subsection{Partial Failure}

Partial failure bestaat uit twee punten:

\begin{itemize}
	\item Allereerst is er de kans dat een systeem kan wegvallen. Sommige onderdelen van een netwerk kunnen werken, terwijl andere onderdelen down zijn of niet meer in gebruik. Hoe meer computers, hoe groter de kans dat één systeem (heel even) wegvalt.
	\item Als tweede punt kunnen andere systemen niet zien wanneer een systeem wegvalt. Het probleem wordt pas opgemerkt wanneer er geen antwoord is. De oorzaak kennen we niet. Dit kan liggen aan: overbelasting, defunct, te traag vergeleken met andere systemen, etc.
\end{itemize}

\subsection{Niet-betrouwbare netwerken}

Bij een asynchrone verbinding moet er met tijdsaannames worden rekening gehouden. Boodschappen worden verstuurd zonder tijdsaannames. Voor een systeem is de tijdsduur en tijdsstip waarop een bericht aankomt irrelevant. 

\begin{itemize}
	\item De oorzaak van een netwerkfout is niet evident. De oorzaak achterhalen is moeilijk, want er zijn drie mogelijke problemen:
	\begin{itemize}
		\item een probleem tussen zender en ontvanger
		\item de ontvanger kan niets ontvangen 
		\item de ontvanger kan niets versturen
	\end{itemize}
	\item Het testen van een verbinding gebeurt met \textit{pingen}. \textit{Exponential back-off} vermijdt het gehaast pingen. De tijd waarop een systeem op een antwoord wacht, zal exponentieel vergroten. Bij de eerste poging wordt er twee seconden gewacht, vervolgens vijf seconden, daarna tien seconden tot uiteindelijk hoogstens vijf minuten. Berichten kort achter elkaar sturen is uit den boze, want dit belast het netwerk en de situatie verergert. Eenmaal de capaciteit van de wachtrij wordt behaald, dan zal de vertraging (in seconden) exponentieel verhogen.
\end{itemize}

\subsection{Niet-betrouwbare tijdsindicaties}

Causality is achterhalen wanneer een event werd uitgevoerd. Concensus is wanneer alle knopen (of nodes) met elkaar overeenkomen bij een beslissing. Er is een onderscheid tussen real-time en monotonische tijd. 

\begin{itemize}
	\item Real-time tijd zijn klokken die gesynchroniseerd worden met het gebruik van een gecentraliseerde server.
	\item Monotonische klokken zijn klokken die op een vast moment starten en enkel vooruit gaan. Er is geen synchronisatie tussen de verschillende systemen. Bij monotonische tijd komen leap-seconden ook voor. Zo is een minuut niet altijd 60 seconden, want soms kan dit 59 of 61 seconden zijn.
\end{itemize}

\subsection{Onderlinge onzekerheid}

Nodes in een gedistribueerd systeem kan enkel veronderstellingen maken. De informatie dat een node bijhoudt verandert regelmatig. Voorbeelden hiervan zijn: klokken die desynchroniseren of nodes die niets terugsturen terwijl ze een update uitvoeren.

\subsubsection{Split-brain}

Split-brain is een concept rond inconsistente data-opslag. De ene helft van het systeem denkt dat iets juist is, terwijl de andere helft van het systeem denkt dat iets anders juist is. Bijvoorbeeld deel A denkt dat systeem 1 de baas is, terwijl deel B denkt dat systeem 2 de baas is.

\subsubsection{Tweegeneralenprobleem}

Het tweegeneralenprobleem bouwt verder op het split-brain concept. Dit weergeeft een scenario waarin beide partijen enkel winnen als ze samenwerken. Elk ander scenario leidt tot verlies. Beide partijen weten niet of de andere partij iets wilt ondernemen. Ze moeten het eerst vragen. De boodschap kan mogelijks niet tot de generaal komen. Dit probleem kunnen we toepassen binnen een online webshop. Er zijn vier gevallen:

\begin{itemize}
	\item Als een online shop het pakket niet verstuurd en de klant betaalt geen geld, dan is er geen probleem.
	\item Als een online shop het pakket wél verstuurd en de klant betaalt geen geld, dan is de shop hier nadelig.
	\item Als een online shop het pakket niet verstuurd en de klant betaalt wél, dan is de klant hier nadelig.
	\item Als een online shop het pakket wél verstuurd en de klant betaalt wél, dan is iedereen \textit{happy}.
\end{itemize}}

\section{Replication en partitioning}

\subsection{Partitionering}

Bij partitionering wordt een groot bestand onderverdeelt over meerdere \textit{nodes} of knooppunten. Op deze manier moet niet alles op één plek worden opgeslaan. Als één van de nodes niet bereikbaar is, dan is het volledige bestand niet toegankelijk. Elk stuk data behoort tot precies één partitie. Bij Mongo en ElasticSearch noemt één partitie een \textit{shard}.

\subsection{Replication}

Replicatie is het maken en onderhouden van verschillende kopieën op meerdere knooppunten. Hiermee wordt redundantie aangeboden. Als  data op node A niet beschikbaar is, dan worden gebruikers doorverwezen naar node B. Dit concept komt vaak voor bij geografisch gespreide netwerken. Bijvoorbeeld een knooppunt in Oceanië, Azië, Europa, etc. 

\subsubsection{Leader-follower technieken}

Er zijn hier drie verschillende leader-follower technieken: 

\begin{enumerate}
	\item  Bij single-leader doen alle clients wat de leider zegt. Alle writes komen vanuit één leider binnen één partitie. De boodschap van de clients is mogelijks achterhaald. Als gevolg worden acties uitgevoerd die niet meer van toepassing zijn. 
	\item Bij leaderless replication versturen de clients elke write naar verschillende nodes. De clients lezen parallel.  Zij zorgen ervoor dat de data OK blijft. Elk verstuurt boodschappen door naar de nabije clients. Achterhaalde data kan worden tegengegaan door te werken met timestamps.
	\item  Multi-leader replication bouwt verder op single-leader replication. Meerdere nodes worden in verschillende datacenters geplaatst.
\end{enumerate}

\subsubsection{Replication-lag}

Leaders zijn niet statisch. \textit{Position-switches} gebeuren door clients te promoveren tot leader. Als een leader verandert, dan is er kans op replication-lag. Dit is een marge dat de leader heeft ten opzichte van andere volgers. In dat geval moet de rest ook meer moeite doen om veranderingen in te halen.

\subsubsection{Het verloop van replication}

Replication kan synchroon of asynchroon verlopen. 
\begin{itemize}
	\item Bij synchrone replicatie wordt er gewacht op de antwoorden van de volgers. Er is garantie dat de data niet zal verloren gaan, met een trager systeem tot gevolg.
	\item Asynchroon is wanneer er niet wordt gewacht op de volgers. Alle overplaatsingen zullen vlot verlopen als er geen wissels gebeuren bij de leaders. Hoe groter de replication lag, hoe groter de kans op dataverlies.
\end{itemize}

 
\begin{figure}
 	\includegraphics[width=\linewidth]{../images/Screenshot_213.png}
 	\caption{Het verschil tussen synchrone en asynchrone replication. Bij synchroon wordt er niet op bevestiging gewacht. Synchrone replicatie gaat direct door naar de target storage. Asynchrone replication wacht op bevestiging van de volgers op de source.}
\end{figure}

\subsubsection{Replicatiefouten}

Fouten bij synchrone replicatie zijn minder prevalent, maar de techniek kan gedwarsboomd worden. Zo is er nog steeds een probleem waarbij een \textit{write}-operation niet kan worden afgewerkt als één van de volgers niet online is. Er wordt gewacht op de bevestiging van de volgers. De twee vaak voorkomende fouten bij asynchrone replicatie zijn \textit{monotonic read} en \textit{read-after-write}. 

\begin{figure}
	\includegraphics[width=\linewidth]{../images/Screenshot_160.png}
	\caption{Voorbeeld van een fout bij synchrone replicatie.}
\end{figure}


\subsubsection{Read-after-write}
Read-after-write (RAW) duidt, zoals de naam het aangeeft, op een fout bij het lezen. De client heeft in dit geval een comment geplaatst en vervolgens wordt er een bevestiging gegeven aan de client. Daarna wilt dezelfde gebruiker dezelfde post inlezen, maar die is nog niet opgeslaan door een andere volger. In een ideaal scenario staat de output gelijk aan de data in de databank. Bij RAW is het belangrijk om met een timestamp te werken. Voorzie daarom bij een schrijfoperatie dat alles tot een specifieke timestamp is voldaan. Zo ja, haal de gegevens op en geef ze aan de client. Zo niet, wacht of kijk naar een andere volger.

\subsubsection{Monotonic read}Een gelijkaardig, maar nog steeds verschillend probleem, is monotonic read. De gebruiker leest een post of comment, maar na een refresh is deze comment opeens niet beschikbaar of niet-bestaand. De volger loopt hier achter op de andere volgers. De klok bij de ene volger loopt voor op de andere. Het verschil hier is dat de gebruiker de tekst niet heeft geschreven, wat wel het geval is bij RAW. Dit lossen we op door de gebruiker altijd van dezelfde replica te laten lezen. Hieronder leest de gebruiker eerst van de volger mét het resultaat. Daarna probeert de gebruiker dit opnieuw, maar bij een volger die achterloopt.


\begin{figure}
	\includegraphics[width=\linewidth]{../images/Screenshot_213.png}
	\caption{Monotonic read}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{../images/Screenshot_214.png}
	\caption{Read-after-write probleem.}
\end{figure}

\subsection{Replicatie en partitionering combineren}

\begin{figure}
	\includegraphics[width=\linewidth]{../images/Screenshot_215.png}
	\caption{Vier partities: elke leader heeft twee volgers. In de volgende foto zijn er vier nodes. Elke node heeft drie onderdelen. Over de vier nodes zijn er vier verschillende partities verdeeld. De replica's of volgers worden achterhaald aan de hand van de stream. De leider van partitie 1 in node 1. De replica's zijn in Node 3 en in Node 4. De leider van partitie 2 is in node 3. De replica's zijn in Node 1 en in Node 2.}
\end{figure}

We kunnen niet zomaar data in stukken snijden. We moetne hotspots vermijden. Een hotspot is een plaats waar de verdeling geen goede verhouding heeft voor iedere node. Hiervoor hebben we twee oplossingen: key-value partitionering en hash partitionering.

\begin{itemize}
	\item Bij key-value partitionering wordt een zo eerlijk en even mogelijke verdeling gemaakt over alle nodes. Alle sleutels binnen een node behoren tot een range. Alle sleutels worden gesorteerd, bijvoorbeeld alles van A t.e.m. E. Afhankelijk van de context wordt vaak voorkomende data binnen dezelfde partitie opgeslaan. Dit zorgt voor meer verkeer op partitie A-D vergeleken met X-Z. De ene partitie zal een hotspot worden, maar de andere zal geen verkeer krijgen.
	\item Hash partioning lost dit probleem merendeels op, maar het is niet de meest efficiënte implementatie. Hier raakt sortering verloren. De afweing hier is dat het toewijzen van de partitie op een afgewogen manier zal gebeuren. De hash houdt rekening met beschikbare plaats. De kans dat een partitie niet gebruikt zal worden is kleiner. 
\end{itemize}


\section{Request routing}

De plaats van data achterhalen kan op drie manieren:

\begin{enumerate}
	\item Iedere node bevat metadata. De client kan een willekeurige knoop contacteren. De knoop weet waar het te zoeken woord is. Hieronder geeft knoop 0 mee dat het te zoeken woord op knoop 2 is.
	\item Er is een laag tussen de client en de knopen. De routing-tier bevat metadata.
	\item De client heeft directe toegang tot de metadata. Dit wordt het minste gebruikt.
\end{enumerate}}

\begin{figure}
	\includegraphics[width=\linewidth]{../images/Screenshot_216.png}
	\caption{De drie mogelijkheden om de plaats van data te achterhalen.}
\end{figure}

De locatie van metadata onderhouden gebeurt met de coordination service. Dit zorgt voor het onderhoud en de mapping van de metadata. Het is de routing tier (RT) tussen de client en de knopen. De RT is direct verbonden met zowel de knopen, alsook met de Zookeeper. Als er iets verandert in de data van een node, dan meoten de nodes dit laten weten aan de Zookeeper.

\begin{figure}
	\includegraphics[width=\linewidth]{../images/Screenshot_217.png}
	\caption{Zookeeper.}
\end{figure}

\chapter{Hadoop Filesystem}

\section{Inleiding}

Hadoop gebruikt parallellisatie om gegevens over meerdere computers in een cluster heen te verdelen, waardoor het verwerkingsproces versnelt. Dit kan worden gebruikt voor het analyseren van gegevens, zoals het ontdekken van trends en patroonherkenning. Hadoop is één van de eerste frameworks voor Big Data Processing. Het is een relatief oud project met \textit{clunky} technieken. Hadoop is ontworpen om clusters te kunnen draaien op normale hardware. Als een cluster bestaat uit honderden computers, dan is de kans groot dat er één zal breken. Het basisidee van Hadoop is om dit soort fouten af te handelen en zodat het systeem blijft werken zoals voordien.

\subsection{Hadoop stack}

Een pure hadoop-stack bestaat uit vier onderdelen:

\begin{itemize}
	\item \textit{Hadoop common}: de gedeelde bibliotheken die door de andere modules worden gebruikt. Dit gedeelte is als gebruiker niet met het blote oog te zien. Het is de meeste onderste laag.
	\item \textit{HDFS} is een filesystem. Dit zorgt voor de distributed file storage. De vertraging is niet op te merken.
	\item \textit{MapReduce} is het processing-gedeelte. MapReduce laat toe om parallel grote datasets te verwerken.
	\item \textit{YARN} voorkomt dat één element in een cluster alle resources zal opeten. Dit element staat in voor request-afhandeling van resourcevragen.
\end{itemize}

\section{Hadoop filesystem}

\subsubsection{Algemeen}

Een filesystem laat toe om data op te slaan en weer op te halen. Er zijn drie soorten data: files, mappen en metadata. Metadata is de info over de mappen of bestanden, zoals file length en permissies. Een filesystem zorgt ervoor dat de data toegankelijk is. De consistentie in filesystems wordt bewaard door middel van een logsysteem. 

\begin{figure}
	\includegraphics[width=\linewidth]{images/hadoop-ecosystem-layers.png}
	\caption{Het Hadoop ecosysteem. Onderaan staat Hadoop FileSystem. Daarboven is er YARN, wat instaat voor het resourcemanagement. Boven het filesystem en de resourcemanager staan er de processing services, waaronder Spark en MapReduce. API's komen in deze samenvatting niet aan bod.}
\end{figure}

\subsubsection{Designprincipes}

De data wordt opgeslaan in een cluster van gewone machines. De focus van HDFS ligt op riante hoeveelheden data op te slaan. Er wordt gewerkt met veel groepen van machines. Als er één machine zou kapot gaan, dan neemt een andere machine over. Er is weinig vertraging. Write-once-read-many-times (WORM) betekent dat gegevens één keer worden geschreven en vervolgens vele keren worden gelezen, maar niet wordt gewijzigd of verwijderd. HDFS ondersteunt deze techniek om zo een hoge fouttolerantie te kunnen bieden en doorvoer te kunnen bieden. Er zijn twee algemene redenen waarom er geen bestanden worden gewijzigd of verwijderd.

\begin{itemize}
	\item HDFS is bedoeld om grote hoeveelheden data op te slaan. Verwijderen en wijzigen vergt kostbare energie. Daarnaast is HDFS fouttolerant, want het systeem moet blijven werken zelfs al vallen er nodes uit. Als een bestand wordt verwijderd, dan beïnvloedt dit de integriteit van de data.
	\item HDFS is bedoeld om een historiek uit te bouwen. De data wordt ingezet in data-analyse, waar nauwkeurigheid een rol speelt, dus er mogen geen wijzigingen aan de data worden aangebracht.
\end{itemize}

\subsubsection{Write-once, read-many-times}

Het principe is 'write-once, read-many-times'. Een bestand wordt eenmalig gemaakt. De doorvoer van het systeem is hier belangrijker. De hoeveelheid verwerkte data per tijdseenheid. HDFS op een klassiek filesysteem. Eens de machine bezig is, kan de doorvoer groot zijn. Er is sprake van \textbf{append-only fashion}. De gegevens kunnen enkel achteraan worden toegevoegd en niet tussenin of helemaal vooraan. Deze werkwijze helpt om de prestaties en betrouwbaarheid van HDFS te verbeteren, omdat het aantal wijzigingen aan gegevens beperkt. 

\subsubsection{Anti-patterns}

Er zijn drie situaties waarin HDFS \textit{overkill} is als oplossing:

\begin{itemize}
	\item Wanneer snelheid een grote rol speelt. De latency is hier niet minimaal.
	\item Wanneer het merendeel van de data uit kleine bestanden bestaat. De nadruk bij HDFS ligt op grote bestanden. De NameNode houdt de directorystructuur bij, waardoor veel kleine bestanden zal leiden tot veel verschillende takken.
	\item Als er meerdere schrijvers op eenzelfde moment nodig zijn. Als één client schrijft en de andere wilt lezen, dan kan er een conflict ontstaan bij de integriteit en het delen van een bestand.
	\item Hadoop maakt gebruik van een append-only fashion. Als de inhoud vooraan of in het midden moet worden toegevoegd, dan zal dit niet lukken in Hadoop. Bij Hadoop kan er enkel achteraan worden toegevoegd.
\end{itemize}


\section{Hadoop-onderdelen}

HDFS heeft twee componenten:

\begin{itemize}
	\item De NameNode.
	\item De DataNode.
\end{itemize}

Kort samengevat is de namenode de node dat het bestandssysteem beheert en bijhoudt waar alle bestanden zijn opgeslagen. De datanode slaat de bestanden op en zorgt ervoor dat gegevens beschikbaar zijn voor lezen en schrijven.

\subsection{NameNode}

Een NameNode (NN) is de centrale coördinator. weet uit welke blokken de data bestaat en houdt bij op welke datanodes de blokken staan. Locaties worden niet persistent bijgehouden. Onderling weten ze dit door middel van \textit{heartbeats}.  Een bestand hernoemen is mogelijk. Bij een cluster is er altijd één NN.

De NN heeft de volgende rollen:

\begin{itemize}
	\item Het beheerde de filesystem namespace.
	\item Het koppelt de datablokken aan de DataNodes.
	\item Het handelt de filesystemverzoeken, zoals het openen/sluiten en hernoemen van bestanden of mappen.
	\item Het gidst de client naar de best passende DataNode.
\end{itemize}

\subsubsection{Opslag voor metadata}

De NameNode slaat metadata over het filesystem op in twee bestanden: de namespace image en de edit log. De \textit{namespace image} bevat informatie over de structuur van het filesystem (metadata), waaronder de directoryboom en de bestanden en mappen die het bevat. De metadata wordt in het RAM bijgehouden. Daarmee is het snel, maar vluchtig. Terwijl het systeem loopt wordt het opgebouwd, maar de informatie is niet persistent. 

De \textit{edit log} bevat een opname van alle wijzigingen in het filesystem, zoals het maken of verwijderen van bestanden en mappen. Dit is een append-log. Telkens als er iets verandert, dan wordt er informatie toegevoegd. Als de NN opnieuw zou opstarten, dan wordt de image en edit log gelezen. Daarna worden de veranderingen van de edit log toegepast op de namespace image om zo een nieuwe namespace image te maken. Daarna wordt een nieuwe edit log gestart.

\subsubsection{Werking NameNode}

De NameNode communiceert met de DataNodes om bij te houden op welke DataNodes welke datablokken worden opgeslagen. Daarnaast zorgt de NN ervoor dat de data beschikbaar en consistent is over een cluster. Wanneer een client data in HDFS wil raadplegen, stuurt hij een verzoek naar de NameNode, die vervolgens bepaalt op welke DataNode de gevraagde datablok staat en de client naar die DataNode stuurt.

\subsection{DataNode}

De DataNodes (DN) stellen de servers voor die de werkelijke datablokken opslaan. Ze zijn verantwoordelijk voor het afhandelen van lees- en schrijfverzoeken voor het maken/verwijderen en hermaken van datablokken. Deze acties gebeuren volgens de instructies van de NN. Alle instructies worden gegeven met een \textit{block report}, wat een periodieke rapportering is van de NN.

Een DN heeft de volgende functies:

\begin{itemize}
	\item Direct luisteren naar instructies van de NN.
	\item Lees- en schrijfverzoeken afhandelen.
	\item Het maken, verwijderen of repliceren van datablokken.
\end{itemize}

Een DN is het werkpaard van een HDFS-cluster, want ze beheren en slaan de data op die in een cluster kan worden teruggevonden. In het werkveld zijn er riante hoeveelheden DN's aanwezig. Alles wat de client leest is afkomstig van een DN. Elk blok heeft een replicatiefactor. De replicatiefactor wijst op hoeveel systemen het bestand beschikbaar moet staan. Een HDFS heeft veel DataNodes.

\subsubsection{Werking DataNode}

Wanneer een client data in HDFS wil raadplegen, stuurt hij een verzoek naar de NameNode, die vervolgens bepaalt op welke DataNode de gevraagde datablok staat en de client naar die DataNode stuurt. De DataNode haalt vervolgens de gevraagde datablok op en stuurt deze terug naar de client.

\begin{figure}
	\includegraphics[width=\linewidth]{images/hdfs-components-namenode-datanode-datanode.png}
	\caption{De verschillende componenten van HDFS, waaronder DataNodes en NameNodes.}
\end{figure}

\section{Single-point-of-failure}

Als één NN uitvalt, of als de NS Image uitvalt, dan zullen de datablokken niet meer toegankelijk zijn. Alle data zal wél blijven bestaan op de DN. Alle blokken hebben random verwijzingen. Een client is niet bewust waarvan de data komt en waartoe die zal gaan. 

\subsection{Bescherming NameNode}

Er zijn twee manieren om het NN te beschermen:

\begin{itemize}
	\item De meest voor de hand liggende manier is om regelmatig een backup te nemen. Hadoop kan worden geconfigureerd om metadata op meerdere filesystems op een synchrone en atomaire manier te schrijven. 
	\item De tweede optie is een secondary NN. Zolang een systeem niet wordt herstart, dan wordt de edit log langer en groter. Dit is niet ideaal, want als het neemt zowel plaats in alsook zal het opstarten van een NS image langer duren. In latere versie van Hadoop hebben ze een secondary NN toegevoegd. Een secondary NN is een proces dat op een andere computer loopt en dat de veranderingen van de edit log verwerkt in de huidige namespace.
\end{itemize}}

\section{HDFS Blokken}

Datablokken hebben niets te maken met "blokken" op een traditionele harde schijf. De datablokken op een Hadoop FS verwijzen naar de stukken data waarin een bestand wordt verdeeld voor opslag. De standaardgrootte van een bestand is 128MB, dus relatief groot. Door een bestand in blokgroottet te verdelen, kan het in HDFS worden opgeslagen, zelfs als het groter is dan elke enkele schijf in een cluster. Het laatste blok van een bestand kan kleiner zijn dan de standaardblokgrootte als het bestand niet een exact veelvoud is van de blokgrootte.

\subsubsection{Preventie}

Om dataverlies te voorkomen, wordt voor elk bestand in HDFS een replicatiefactor ingesteld. Deze waard steld het aantal replicants voor. Een replicant is een 'dubbel' van een bestand. HDFS zal \textit{proberen} om het aantal replicanten te laten overeenkomen met de opgestelde replicatiefactor. Verschillende bestanden kunnen afwijkende replicatiefactoren hebben. De factor hangt af van de gewenste redundantie en bescherming.

\section{Anatomy}

\subsection{Anatomy of a File Read}

Het proces om een bestand uit HDFS te lezen, omvat de volgende stappen:

\begin{enumerate}
	\item De Client maakt een oproep naar een instantie van het HDFS.
	\item De instantie maakt gebruik van Remote Procedure Calls (RPC) naar de NN om de lcoaties van de eerste paar blokken in het bestand te achterhalen. Voor elke blok geeft de NN de adressen van de DN's die een kopie hebben van dat blok. Die adressen zijn gesorteerd op afstand van de DN tot de client. De HDFS geeft een \textit{DataInputStream}-object terug naar de client. Dit object bevat een \textit{DFSInputStream}-object, die de Input/Output tussen de DN en de NN beheert.
	\item De client roept een leesoperatie op de stream. Het \textit{DFSInputStream}-object verbindt automatisch met de DN voor de volgede blok. Dit gebeurt transparant met de client, wie denkt dat het een continue stream aan het lezen is. 
	\item Eenmaal de client klaar is met lezen, dan stuurt de client een een \textit{close} op de \textit{FSDataInputStream}.
\end{enumerate}

De client contacteert de DN direct om data op te halen. De NN gidst de client richting de meest optimale verbinding met een datablok. De NN geeft géén data, maar het verleent enkel richting door de locaties van de block requests terug te geven. Deze locaties staan in het RAM-geheugen, daarom gebeurt dit proces heel snel.

\subsubsection{Rollen}

\begin{itemize}
	\item De \textit{DistributedFileSystem} is een Java-klasse die een interface voorziet om met het HDFS te kunnen interrageren. Met dit interface kan een client toegang tot een bestand krijgen of een bestand manipuleren. De DFS communiceert met de NN om acties uit te voeren. De DFS gebruikt RPC's om verzoeken naar de NN te versturen en te ontvangen. De meest gebruikte functies zijn: 
	\begin{itemize}
		\item 'open' om een bestaand bestand te openen: een inputstream wordt teruggegeven.
		\item 'create' om een nieuw bestand aan te maken: een outputstream wordt teruggegeven om data in het bestand te schrijven
		\item 'delete' om een bestand te verwijderen
		\item 'mkdirs' om een nieuwe directory te maken
		\item 'listStatus' om een lijst van bestanden en directory statussen, van een gespecifeerd pad, terug te krijgen
	\end{itemize}
	\item De DFSInputStream is een interne klasse dat gebruikt wordt om data uit een bestand te lezen. Het verbindt met het passende DN, daarnaast streamt het de data terug naar de client.
	\item De DataInputStream is een klasse dat een interface voorziet om datatypes uit te lezen. Het wordt gebruikt om data van eender welke inputstream te lezen, inclusief een DFSInputStream.
	\item Een FSDataInputStream is een klasse die de DFSInputStream inkapselt en extra functionaliteit aanbiedt. Het wordt gebruikt om eender welke data te lezen van een bestand in HDFS.
\end{itemize}


\begin{figure}
	\includegraphics[width=\linewidth]{images/HDFS-Read.png}
	\caption{File read schema opgehaald van Javatpoint (2022)}
\end{figure}

\subsection{Anatomy of a File Write}

Het proces om een nieuw bestand in HDFS te maken, omvat de volgende stappen:

\begin{enumerate}
	\item De client roept een \textit{create} of \textit{delete} aan op de \textit{DistributedFileSystem}.
	\item Het HDFS maakt een RPC-oproep om een nieuw bestand in de namespace van het filesystem te maken, zonder blokken die eraan gekoppeld zijn. De NN voert controles uit, waaronder kijken of het bestand bestaat of kijken naar de machtigingen van de client. Als alle controles slagen, dan maakt de NN een record van het nieuwe bestand. Zo niet, dan faalt de \textit{file creation} en dan krijgt de client een \textit{IO-exception} te zien.
	\item Het HDFS geeft een \textit{FSDataOutputStream} terug aan de client. Dit object bestaat uit een \textit{DFSOutputStream}, die de communicatie met de DN's en de NN beheerst.
	\item Als de client gegevens schrijft, dan wordt de \textit{DFSOutputStream} in pakketten gesplitst. Deze pakketten worden naar een \textit{data queue} geschreven. Deze wachtrij wordt doorgestuurd naar een \textit{DataStreamer}. Dit object vraagt om beurt aan de NN om nieuwe blokken te alloceren, dit met een lijst van geschikte DN's om de replicaties op te slaan.
	\item De lijst van DN's vormt een pipeline.
	\item Het \textit{DataStreamer}-object streamt de pakketten naar de eerste DN in de pipeline, die elk pakket opslaat en doorstuurt naar de tweede DN in de pipeline. De tweede DN herhaalt dit, de derde DN herhaalt dit, enzovoort.
	\item De \textit{DFSOutputStream} onderhoudt een \textit{acknowledgement queue}. Dit terwijl pakketten worden bevestigd door de DN's. De pakketten worden van de \textit{ack queue} verwijderd als ze door alle DN's in de pipeline zijn bevestigd.
	\item Eenmaal de client klaar is met het schrijven van gegevens, roept de client \textit{close} aan op de stream. Dit spoelt alle overblijvende pakketten naar de DN-pipeline en wacht op bevestigingen voordat hij contact opneemt met de NN. De NN weet al uit welke blokken het bestand bestaat, dus deze node moet enkel wachten tot de blokken gerepliceerd zijn voordat de node succesvol terugkomt.
\end{enumerate}

\subsubsection{Rollen}
\begin{itemize}
	\item De \textit{DistributedFileSystem} is een Java-klasse die een interface voorziet om met het HDFS te kunnen interrageren. Met dit interface kan een client toegang tot een bestand krijgen of een bestand manipuleren. De DFS communiceert met de NN om acties uit te voeren. De DFS gebruikt RPC's om verzoeken naar de NN te versturen en te ontvangen. De meest gebruikte functies zijn: 
	\begin{itemize}
		\item 'open' om een bestaand bestand te openen: een inputstream wordt teruggegeven.
		\item 'create' om een nieuw bestand aan te maken: een outputstream wordt teruggegeven om data in het bestand te schrijven
		\item 'delete' om een bestand te verwijderen
		\item 'mkdirs' om een nieuwe directory te maken
		\item 'listStatus' om een lijst van bestanden en directory statussen, van een gespecifeerd pad, terug te krijgen 
	\end{itemize}
	\item De \textit{DataStreamer} is verantwoordelijk voor het streamen van data vanuit de client naar de verschillende DN's in een cluster. Het wordt gebruikt wanneer een gebruiker wilt schrijven naar een bestand in een HDFS. Dit object vraagt de blokallocaties aan de NN, zo wordt er een lijst bijgehouden van geschikte DN's.
	\item Een \textit{DFSOutputStream} is een interne klassie die gebruikt wordt om data te schrijven naar een bestand. Het splitst de data in pakketten, streamt de pakketten naar de DN's en verzekert dat de data op een betrouwbare manier wordt afgehandeld.
	\item De \textit{FSOutputStream} is een publieke lasse dat een interface voorziet voor clients om data uit te schrijven naar een bestand in een HDFS. Het inkapselt de \textit{DFSOutputStream} en synchroniseert de data stream.
\end{itemize}


\begin{figure}
	\includegraphics[width=\linewidth]{images/HDFS-Write.png}
	\caption{File Write schema opgehaald van Javatpoint (2022)}
\end{figure}

\subsubsection{Foutafhandeling}

Bij het falen van een DN, terwijl er data naartoe wordt geschreven, dan worden er enkele acties afgehandeld. De client weet wat er achter de schermen gebeurt bij zo een afhandeling. Sommige blokken kunnen \textit{under-replicated} zijn, maar de NN zal dit opmerken. De blokken zullen asynchroon worden gerepliceerd in de cluster. Dit proces herhaalt zich tot de replicatiefactor is behaald.

\section{Command-Line}

De CLI voor HDFS biedt een reeks commando's aan om te communiceren met een HDFS-cluster. Deze taal heeft gelijkenissen met de Bash-taal. Om een lijst te krijgen met beschikbare commando's en opties, moet het commando "hadoop fs -help" worden ingegeven.

\subsubsection{Kopiëren en ophalen}

\begin{lstlisting}[language=Hadoop]
// Lokaal --> HDFS
hadoop fs --copyFromLocal input/docs.txt hdfs://localhost/user/dylan/docs.txt

// Alles in een Hadoop map bekijken
hadoop fs -ls .

// HDFS naar Lokaal
hadoop fs --copyToLocal hdfs://localhost/user/dylan/docs.txt output/docs.txt
\end{lstlisting}

\subsubsection{Nieuwe elementen aanmaken}
\begin{lstlisting}
// Bestand aanmaken
hadoop fs -touch docs.txt

// Directory aanmaken
hadoop fs -mkdir input
\end{lstlisting}

\section{MapReduce}

Hadoop MapReduce is een programma dat wordt gebruikt voor het verwerken van grote hoeveelheden gegevens in een distributiefile-systeem. Het maakt gebruik van parallellisatie om de gegevens te verdelen over meerdere computers in een cluster, waardoor het verwerkingsproces sneller wordt. Dit kan worden gebruikt voor het analyseren van gegevens, zoals het ontdekken van trends en patroonherkenning. Een MapReduce bestaat uit drie fasen:

\begin{enumerate}
	\item Mapping
	\item Shuffle
	\item Reduce
\end{enumerate}

\subsubsection{Voordelen van MapReduce}

MapReduce vereist geen kennis van parallelisatie, data distributie en fouttolerantie binnen het programmeren. Dit gebeurt automatisch over een riant aantal machines gespreid. Alle details rond partitionering van de input-data, scheduling en foutafhandeling wordt al afgehandeld. Bij MapReduce kan de programmeur focussen op de logica van de applicatie en het is zo makkelijker om parallele programma's te schrijven die schaalbaar, efficiënt en fouttolerant zijn. Bij MapReduce is \textit{re-execution} het belangrijkste mechanisme. Als een taak faalt, dan zal het framework automatisch de taak opnieuw proberen op een nieuwe machine.

\subsection{Data Flow}

 Het framework zal iedere split aan een \textit{map task} toekennen. Deze task

\begin{enumerate}
	\item Bij een MapReduce job wordt de inputdata over verschillende \textit{chunks} of \textit{inputsplits} heen verdeeld. De \textit{map job} zal het deel lijn per lijn bekijken. Iedere mapper zal een deel van het bestand te zien krijgen.
	\item Het framework zal iedere split aan een \textit{map task} toekennen. De \textit{map task} verwerkt de data en genereert een set van key-value paren.
	\item Het framework verzamelt en groepeert de waarden met dezelfde key. Deze groeperingen worden doorgegeven aan de \textit{reduce task}.
	\item De \textit{reduce task} verwerkt de keys met hun waarden. De output is een verzameling van key-value paren. Deze output wordt naar een outputbestand in HDFS geschreven.
\end{enumerate}

\subsection{Mapping}

De Mapper in Hadoop MapReduce is een programma dat wordt gebruikt om gegevens te verdelen over meerdere computers in een cluster, zodat ze parallel kunnen worden verwerkt. De Mapper leest de gegevens in en verdeelt ze in kleinere stukjes, die vervolgens naar de verschillende computers in het cluster worden gestuurd om te worden verwerkt. Dit maakt het mogelijk om grote hoeveelheden gegevens snel te verwerken en te analyseren. De Mapper is het eerste onderdeel van het MapReduce-proces en zorgt ervoor dat de gegevens op een gestructureerde manier worden verwerkt.

\subsubsection{Voorbeeld luchttemperatuur}

De mapfunctie zal ieder inputrecord verwerken. Ieder record bestaat uit lijnen tekst. Bij het verwerken worden de jaar- en luchttemperatuurvelden uit het bestand gehaald. Het filter recors met ontbrekende of verdachte temperaturen. De mapfunctie zal key-value paren voor ieder record gaan genereren. De key is het jaar en de value is de luchttemperatuur. De mapfunctie geeft de key-value paren aan de shuffle \& sort fase.

\subsection{Shuffling}

De Shuffle-fase in Hadoop MapReduce is een belangrijk onderdeel van het MapReduce-proces waarbij de gegevens worden verzameld en gerangschikt op basis van de sleutels die aan de gegevens zijn toegekend. De Mapper verdeelt de gegevens in kleinere stukjes en stuurt deze naar de verschillende computers in het cluster, waar ze worden verwerkt. De Reducer verzamelt vervolgens de verwerkte gegevens van alle computers in het cluster en sorteert ze op basis van de sleutels, zodat de gegevens kunnen worden verwerkt en geanalyseerd. De Shuffle-fase is dus een cruciale stap in het MapReduce-proces omdat het ervoor zorgt dat de gegevens op een gestructureerde manier worden verwerkt en geanalyseerd.

\subsection{Reduce}

In een MapReduce job zal de input van de reducefunctie uit key-value paren bestaan. Die werden aangemaakt door de mapfunctie en doorgegeven aan de shuffle \& sort fase. De reducefuntie verwerkt iedere key met de toebehorende waarden. Uiteindelijk is de laatste stap in de MapReduce om een output te maken. De reducer wordt bijvoorbeeld gebruikt om totale aantallen te berekenen of gemiddelden te berekenen voor een bepaalde groep gegevens. Het is een belangrijk onderdeel van het MapReduce-proces omdat het ervoor zorgt dat de gegevens op een gestructureerde manier worden verwerkt en geanalyseerd.


\subsubsection{Voorbeeld luchttemperatuur}

De reducefunctie zal, van de shuffle \& sort fase, een verzameling van key-value paren ontvangen. De key is hier het jaar, en de value is de luchttemperatuur. De reducefunctie itereert door de temperatuurwaarden van ieder jaar en het neemt de maximale waarde. Dit hoort tot de finale uitvoer. De finale uitvoer bestaat uit een set key-value paren, waarbij de key het jaar is en de waarde de grootste luchttemperatuur die werd opgenomen in dat jaar.

\section{Java MapReduce}

Java werd gekozen omdat dit de meest prevalente taal is voor MapReduce. Een MapReduce applicatie bestaat uit drie onderdelen:

\begin{enumerate}
	\item Het schrijven van de mapfunctie. De mapfunctie zal iedere input record verwerken en maakt hierop een verzameling van key-value paren. De mapper-klasse wordt ge-\textit{extend} en enkel de \textit{map()}-methode wordt overerfd. De \textit{map()}-methode vraagt een sleutel als inputwaarde op. Dit is regelmatig de lijn waarop tekst voorkomt. Op basis van deze invoer maakt het een verzameling van key-value paren.
	\item Het schrijven van de reducefunctie. De reducefunctie verwerkt iedere key met de toebehorende waarden. De Reducer-klasse \textit{extenden} en de \textit{reduce()}-methode moeten worden overgeërfd. De input bij deze methode is een sleutel en een set waarden dat de verzameling van output key-value paren gaat maken.
	\item Boilerplate-code schrijven. Dit is de code die specifieert hoe de MapReduce job moet draaien. Iedere fase, inclusief extra-reduce fasen, moeten aan bod komen. Alles van job configuratie, input en outputpaden instellen en \textit{job running} moet in de boilerplate code terug te vinden zijn. 
	\item In sommige gevallen is een combinerfunctie handig. Deze lijkt sterk op een reducefunctie en wordt op de output van de \textit{map task} toegepast. De combinerfunctie wordt gebruikt om het aantal data, dat tussen de mapper en de reducer bevindt, aan te passen. Zo wordt het proces efficiënter behandeld. Voor een combiner moet de Reducer-klasse \textit{extenden} en de \textit{reduce()}-methode worden overgeërfd, net zoals bij de reducefunctie.
\end{enumerate}

\begin{figure}
	\includegraphics[width=\linewidth]{images/mapper-reducer-mapreduce-job-flow.png}
	\caption{Een vereenvoudigde versie van de job-flow bij MapReduce.}
\end{figure}

\subsubsection{WordCount-oefening}

\begin{lstlisting}[language=Java]
public class WordCount {
	
	public static class TokenizerMapper
	extends Mapper<Object, Text, Text, IntWritable>{
		
		private final static IntWritable one = new IntWritable(1);
		private Text word = new Text();
		
		public void map(Object key, Text value, Context context
		) throws IOException, InterruptedException {
			StringTokenizer itr = new StringTokenizer(value.toString());
			while (itr.hasMoreTokens()) {
				word.set(itr.nextToken());
				context.write(word, one);
			}
		}
	}
	
	public static class IntSumReducer
	extends Reducer<Text,IntWritable,Text,IntWritable> {
		private IntWritable result = new IntWritable();
		
		public void reduce(Text key, Iterable<IntWritable> values,
		Context context
		) throws IOException, InterruptedException {
			int sum = 0;
			for (IntWritable val : values) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key, result);
		}
	}
	
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf, "word count");
		job.setJarByClass(WordCount.class);
		job.setMapperClass(TokenizerMapper.class);
		job.setCombinerClass(IntSumReducer.class);
		job.setReducerClass(IntSumReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
}
\end{lstlisting}

\subsection{Mapper-klasse}

De mapperklasse is een generieke klasse dat vier typeparameters heeft. De mapfunctie zal bij ieder input record een verzameling key-value paren aanmaken. Hier zal de key het regelnummer zijn. De value zal de toebehorende tekst op de regel zijn.
\begin{itemize}
	\item De input key type is een Object.
	\item De input value type is een Text.
	\item De output key type is een Text. Dit is de volledige tekst dat op een lijn terug te vinden is. representeert het woord waarop er geteld word.
	\item De output value type is een IntWritable. Dit is het aantal voorkomens van het toebehorende woord.
\end{itemize}

\subsection{Reduce-klasse}

De reduceklasse verwerkt iedere key en de toebehorende waardeN. De outputkey zal hier het woord zijn, en de outputvalue zal hier het aantal voorkomens van een woord in een bestand zijn. 

\subsection{Main-methode}

De main-methode zal een Job-object aanmaken. Dit object geeft aan hoe de job moet worden gedraaid. De \textit{setJarByClass}-methode specifieert het JAR-bestand dat de klassen bevat in de job. Hadoop gebruikt deze informatie om het meest passende JAR-bestand terug te vinden. Vervolgens worden de input- en outputpaden toegelicht. 

\subsubsection{Inputpaden}
Het inputpad wordt bepaald door \textit{addInputPath} van het object \textit{FileInputFormat}. De input kan hier één bestand, maar ook meerdere bestanden in een directory zijn. Globbing is ook mogelijk.

\subsubsection{Outputpaden}

Het outputpad wordt bepaald door \textit{addOutputPath} van het object \textit{FileOutputFormat}. De outputfile of directory mag, voor het uitvoeren van de job, niet bestaan! Deze voorzorg wordt genomen om \textit{data loss} te voorkomen.

\subsubsection{Afwerking}

De \textit{waitForCompletion}-methode wacht tot de job is afgerond. Bij het al dan niet succesvol afwerken zal de job een boolean teruggeven. 

\subsection{Development Environment}

\subsubsection{POM-file}
Alle projecten worden van het formaat 'Maven' zijn. Dit wordt vlot in Eclipse aangemaakt. De \textit{dependencies} moeten de nodige MapReduce en Hadoop plugins bevatten. Alle dependencies kan de ontwikkelaar terugvinden in het \textit{pom.xml} bestand. Dit bestand is het configuratiebestand van Maven.

\subsubsection{JAR genereren}

Om te kijken of alle dependencies en properties correct zijn ingesteld, moet de ontwikkelaar een JAR maken. Dit door te rechtermuisklikken op de POM-file, vervolgens moet een nieuwe build worden aangemaakt.  In de uitvoer krijgt de ontwikkelaar een \textit{build failure} of een \textit{build success}.

\subsubsection{JAR in een Hadoop NameNode plaatsen.}

Het JAR-bestand wordt in de vagrantmap geplaatst. Er wordt verondersteld dat de Hadoop-container aan het draaien is. De JAR-file wordt naar een NN-cluster doorgestuurd met de volgende commando's.

\begin{lstlisting}[language=Bash]
// op de vagrantmachine
docker cp target/hadoop-example-1.0-SNAPSHOT.jar namenode:/
docker exec -it namenode bash
ls -l hadoop-example-1.0-SNAPSHOT.jar

// in de hadoop-shell
hadoop fs -mkdir input/ncdc
hadoop fs -copyFromLocal 190? input/ncdc
hadoop fs -ls input/ncdc

// het uitvoeren van de JAR in de hadoop-shell
hadoop jar hadoop-example-1.0-SNAPSHOT.jar be.hogent.dit.tin.MaxTemperature input/ncdc output/ncdc

// resultaat bekijken
hadoop fs -cat output/ncdc/part-r-00000
\end{lstlisting}

\section{YARN}

\subsection{Algemeen}

YARN is een resourcemanager voor Hadoop-clusters. Deze daemon onderhoudt het toewijzen van CPU, geheugen en middelen aan verschillende toepassingen die op de cluster draaien. YARN biedt API's om mideelen op een cluster te verzoeken te gebruiken, maar deze API's worden niet door de gebruiker gebriukt. In plaats daarvan schrijven gebruikers naar \textit{higher-level} API's die door \textit{distributed computing frameworks} worden aangeboden. Voorbeelden hiervan zijn: MapReduce, Spark en Flink. Deze high-level API's verbergen de details van resourcemanagers. Zo kunnen programmeurs focussen op de logica.

\subsubsection{Extra ondersteuning}

YARN biedt ook ondersteuning voor het inplannen en uitvoeren van toepassingen op de cluster, alsook het monitoren van de status van die toepassingen. Gebruikers kunnen zo een groot aantal toepassingen uitvoeren op deen Hadoop-cluster, waaronder \textit{batch processing}, \textit{stream processing}, \textit{machine learning} en \textit{interactive SQL}.

\subsection{Core Services}

YARN biedt \textit{core services} aan van twee types \textit{long-term daemon processes}: 

\begin{itemize}
	\item Er is slechts één resourcemanager (RM) per Hadoop-cluster. Deze is verantwoordelijk voor het beheren van middelengebruik per cluster. De RM ontvangt \textit{resource requests} van \textit{application masters} en bepaalt vervolgens welke nodes op de cluster beschikbare middelen hebben om deze verzoeken te vervullen.
	\item Er is één actieve nodemanager (NM) op elke node van een Hadoop-cluster. De NM is verantwoordelijk voor het monitoren en opstarten van de containers op een node. Een container is een \textit{ligth-weight execution environment} die een toepassingsspecifiek proces uitvoert met een beperkt aantal middelen, zoals geheugen en CPU. Containers in YARN zijn niet gerelateerd aan de containers dat Docker heeft.
\end{itemize}

De RM en NM werken samen om middelen toe te wijzen en applicaties uit te voeren op een Hadoop-cluster. Gebruikers kunnen hun toepassingen aan YARN voorstellen via high-level API's.

\begin{figure}
	\includegraphics[width=\linewidth]{images/yarn-daemons-hadoop-architecture.png}
	\caption{YARN architectuur met daemon services.}
\end{figure}

\subsection{YARN Application run}

\begin{enumerate}
	\item Een client neemt contact op met de RM en vraagt om een \textit{application-master-process} op te starten.
	\item De RM zoekt een NM die de \textit{application-master} in een contianer kan starten. Alles daarna is afhankelijk van de toepassing. Bij een berekening moet er een waarde naar de client worden teruggestuurd. Bij MapReduce moet er een gedistribueerde bewerking worden uitgevoerd, want alle woorden zijn verspreid over verschillende containers. YARN zelf biedt géén manier om onderdelen van een toepassing met elkaar te laten communiceren.
\end{enumerate}

\subsection{Resource requests}

In een YARN resourcemanager wordt een resource request verstuurd om een specifieke taak mogelijk te maken. Deze request bevat een gepast aantal bewerkingsmiddelen, zoals CPU en geheugen. Deze requests bevatten \textit{locality constraints}. Dit geeft aan waartoe de middelen moeten beschikbaar worden gesteld. Als een container een HDFS blok moet verwerken, dan moet de resource request toelichten dat een container op één van de nodes, waar er een replica is, moet worden geplaatst. Zo wordt de data doorvoer geminimaliseerd én de snelheid van de applicatie wordt verbeterd.

\subsection{Scheduling}

De scheduler bepaalt of de middelen beschikbaar zijn en, indien mogelijk, worden de middelen toegekend aan de toepassing. Als een cluster druk is, met andere woorden zijn de middelen niet direct beschikbaar, dan zal de scheduler de \textit{resource requests} tijdelijk \textit{on-hold} plaatsen. Nadien worden de middelen aan de applicatie toegekend.

\subsubsection{Verschillende schedulers}

Er zijn verschillende schedulers in YARN:

\begin{itemize}
	\item FIFO, of \textit{first-in, first-out} zal de middelen aan applicaties toekennen in de volgorde waarin de scheduler de request kreeg. \textit{First come, first served}.
	\item \textit{Capacity scheduler} laat administrators toe om specifieke middelen te reserveren voor gepaste types toepassingen. 
	\item \textit{Fair scheduler} zal de resources op een eerlijke manier proberen te verdelen over alle applicaties heen. Hier zijn er verschillende factoren zoals de noden aan middelen en voorafgaand gebruik voor iedere applicatie.
\end{itemize}

\newpage
\section{MapReduce-oefening}
\subsubsection{Deel I: Mapper, partitioner en combiner}
\begin{lstlisting}[language=Java]
public class InvertedIndex {
	
	public static class InvertedIndexMapper 
	extends Mapper<LongWritable, Text, Text, LongWritable>{
		
		private static final LongWritable ONE = new LongWritable(1);
		
		@Override
		public void map(LongWritable key, Text value, Context context) 
		throws IOException, InterruptedException {
			
			//Bepaal de bestandsnaam
			FileSplit filesplit = (FileSplit) context.getInputSplit();
			Path path = filesplit.getPath();
			String filename = path.getName();
			
			//Itereren over alle tokens, ofwel woorden, in een tekst.
			StringTokenizer itr = new StringTokenizer(value.toString());
			
			while(itr.hasMoreTokens()) {
				String keyToWrite = itr.nextToken().toLowerCase() + "@" + filename;
				context.write(new Text(keyToWrite), ONE);
			}
		}
	}
	
	
	// Combiner:
	// Alle '1's optellen om het verkeer te verminderen.
	public static class InvertedIndexCombiner
	extends Reducer<Text, LongWritable, Text, LongWritable>{
		
		@Override
		public void reduce(Text word, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException{
			long total = 0;
			for(LongWritable value : values) {
				total += value.get();
			}
			
			context.write(word,  new LongWritable(total));
			
		}
	}
	
	// Partitioner: (woord@filename) enkel 'word' hashen
	public class InvertedIndexPartitioner<Text, LongWritable> extends Partitioner<Text, LongWritable>{
		public int getPartition(Text key, LongWritable value, int numReduceTasks) {
			String s = key.toString().split("@")[0];
			return (s.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
		}
	}
	
\end{lstlisting}

\newpage

\subsubsection{Deel II: Reducer}
\begin{lstlisting}[language=Java]
	// Reducer
	public static class InvertedIndexReducer extends Reducer<Text, LongWritable, Text, Text>{
		
		private String previousWord = null;
		private StringBuilder outString = new StringBuilder();
		
		
		// key: word@filename
		@Override
		public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
			String word = key.toString().split("@")[0];
			String filename = key.toString().split("@")[1];
			
			long sum = 0;
			
			if(previousWord != null && !previousWord.equals(word)) { // nieuw woord gezien
				outString.setLength(outString.length() - 1);
				context.write(new Text(previousWord), new Text(outString.toString()));
				
			}
			
			//bezig met huidig woord
			for (LongWritable value:values) {
				sum += value.get();
			}
			
			// outstring aanpassen
			outString.append(filename + ":" + sum + ";");
			
			// previousword aanpassen
			previousWord = word;
		}
		
		
		public void cleanup(Context context) throws IOException, InterruptedException{
			outString.setLength(outString.length() - 1);
			context.write(new Text(previousWord), new Text(outString.toString()));
			super.cleanup(context);
		}
	}
	
\end{lstlisting}

\newpage

\subsubsection{Deel III: Boilerplate}
\begin{lstlisting}[language=Java]
	public static void main(String[] args) throws Exception {
		if(args.length != 2) {
			System.out.println("Usage: Nuclear meltdown. Usage: <input_dir> <output_dir>");
			System.exit(-1);
		}
		
		//instance of job maken
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf, "WordCount");
		
		// moet je altijd doen als je ...
		job.setJarByClass(InvertedIndex.class);
		
		// set mapper and reducer
		job.setMapperClass(InvertedIndexMapper.class);
		job.setReducerClass(InvertedIndexReducer.class);
		
		// set combiner
		job.setCombinerClass(InvertedIndexCombiner.class);
		job.setPartitionerClass(InvertedIndexPartitioner.class);
		
		//set input and output types
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(LongWritable.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(LongWritable.class);
		
		//set input and output directory
		// waar zijn de input-files?
		FileInputFormat.addInputPath(job, new Path(args[0]));
		
		// waar zijn de outputfiles? 
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		// voor het testen
		job.setNumReduceTasks(0);
		
		System.exit(job.waitForCompletion(true) ? 0 : 1);		
	}
}
\end{lstlisting}

\chapter{Kafka}

\subsubsection{Probleemstellingen}

Veronderstel dat er een systeem is met n-aantal bronsystemen en m-aantal doelsystemen. Om de twee met elkaar te linken, zou ieder bronsysteem moeten verbonden zijn met ieder doelsysteem. Dit komt gelijk op hoogstens een \textit{n x m} aantal berekeningen. Elke connectie neemt resources van het systeem. Daarnaast zijn er drie zaken waarmee er rekening mee moet worden gehouden.

\begin{itemize}
	\item Protocol geeft aan hoe de data moet worden getransporteerd.
	\item Het formaat of de manier hoe de data is \textit{geparsed}.
	\item Dataschema en de evolutie van de data: hoe is de data gevormd en hoe kan het veranderen. Wat is de locatie van de data?
\end{itemize}

\subsubsection{Introductie Kafka}

Kafka is platform dat streaming tussen bron- en doelsystemen ondersteund. Tussen de systemen is er een \textit{broker}. Met Kafka als tussenpersoon verandert dit naar een \textit{n + m}-aantal berekeningen.

\subsubsection{Werking}

\begin{enumerate}
	\item Wanneer een bronsysteem data aanmaakt, dan wordt er data verstuurd naar Kafka. Events die de website heeft gestuurd wordt naar Kafka gestuurd. Een doelsysteem weet niet van wie de data komt.
	\item Kafka slaat het bericht op in een topic. Een topic is een logische categorie waarin de message is opgeslaan. 
	\item Het doelsysteem kan dan de message van de topic opgebruiken, verwerken en nodige acties nemen.
\end{enumerate}

\subsubsection{Voordelen Kafka}

\begin{itemize}
	\item Het is een volwassen stuk open-source software.
	\item Het is een gedistribueerd systeem. De architectuur is fouttolerant gebouwd.
	\item De horizontale schaalbaarheid is goed. Op honderden brokers blijft het systeem even efficiënt werken.
	\item De \textit{latency} is minimaal. 
\end{itemize}

\subsubsection{Use cases}

De twee belangrijkste use cases om te weten:

\begin{itemize}
	\item Kafka wordt ingezet bij het verzamelen van metrieken en de bijhorende analyse. Die data wordt verwerkt in een \textit{recommendation}-systeem, bijvoorbeeld bij Netflix, YouTube of LinkedIn.
	\item De voornaamste use case is stream processing, waar er een oneindige stroom van data wordt beheerd.
\end{itemize}


\section{Concepten van Kafka}

\subsubsection{Topic}

Eén datastroom van boodschappen die met elkaar te maken hebben. De structuur is vergelijkbaar met een tabel. Er is geen beperking, buiten hardware-limitaties, op het aantal topics dat in een Kafka-systeem kan worden aangemaakt.

\subsubsection{Partitie}

Een topic is opgedeeld in een aantal partities. het doel van meerdere partities is om de \textit{workload} en doorvoer evenwaardig te verdelen. Het aantal partities staat vast wanneer een topic wordt aangemaakt.

\begin{itemize}
	\item De volgorde ligt definitief vast. De volgorde wordt bepaald door de offset. Het is vergelijkbaar met een oude typmachine. Eenmaal een lijn is geschreven, dan blijft dit zo. De volgorde waarin een partitie wordt geschreven, is dezelfde volgorde waarin een lezer de partitie zal lezen.
	\item Dit is een \textit{append-only} logfile. Als iets geschreven is in de partitie, dan zal dit achteraan zijn. Daarnaast kan een boodschap niet gewijzigd of verwijderd worden. Als er een foute boodschap wordt verstuurd, dan is er een tweede boodschap nodig om de fout recht te zetten.
	\item Een broker kijkt nooit naar de data. Ze plakken er enkel metadata aan, maar ze bewerken geen inhoud.
	\item Er is geen verhouding tussen nummers van verschillende partities. De volgorde en offset is enkel zinvol binnen dezelfde partitie. Offset 3 bij partitie 0 heeft geen invloed of correlatie met offset 3 van partitie 1.
\end{itemize}


\subsubsection{Voorbeeldcasus}

\begin{enumerate}
	\item Iedere vrachtwagen in een vloot rapporteert de locatie van de GPS naar Kafka. Iedere vrachtwagen zal telkens, iedere 20 seconden, naar dezelfde topic schrijven. Het bericht omvat de huidige coördinaten.
	\item De data van dezelfde truck moet in dezelde partitie worden opgeslaan.
	\item Bij verschillende partities is er geen garantie dat een vrachtwagen telkens naar dezelfde partitie zal schrijven.
\end{enumerate}}


\subsubsection{Kenmerken}

Kafka bezit vier belangrijke kenmerken:

\begin{itemize}
	\item De offset is enkel geldig binnen een gekozen topic partitie.
	\item De volgorde is enkel binnen een partitie gegarandeerd. De volgorde waarin een consumer een topic leest is niet gelijk aan de volgorde als hoe ze messages werden gepost.
	\item Data kan opstapelen. Als er niets wordt toegevoegd of gelezen, dan zal Kafka alles ouder dan een week verwijderen. Deze periode kan worden veranderd. Een tweede mogelijkheid is om een capaciteitslimiet in te stellen, bijvoorbeeld wanneer de partitie te groot is.
	\item Data is immutable. Eenmaal geschreven, kan de data niet meer worden aangepast.
	\item Als er geen key wordt meegegeven, dan kiest Kafka zelf een partitie om de data naar uit te schrijven. Deze keuze gebeurt willekeurig om de partities even groot te houden. Op de key wordt er een hash berekend. 
\end{itemize}

\section{Rollen}

\subsection{Broker}

Iedere Kafka-cluster bestaat uit minstens één \textit{broker}.  

\begin{itemize}
	\item Een \textit{broker} is een server met een eigen ID en topic-partitions.
	\item Er is geen specifieke master-broker. Er wordt verbonden met een willekeurige broker die de metadata van de cluster bijhoudt. Verbinden met één broker staat gelijk aan verbinden met de hele cluster.
	\item Bij enkel één broker is er geen fouttolerantie. Pas vanaf twee brokers biedt het systeem fouttolerantie aan. Per standaard zijn er drie brokers.
	\item Het toekennen van een partitie aan een broker gebeurt o.b.v. een algoritme.
\end{itemize}

\subsubsection{Data partitioneren over verschillende brokers}

Er zijn drie brokers en twee topics. Topic A heeft drie partities en topic B heeft twee partities. Kafka probeert altijd om de partities van één topic op verschillende brokers te zetten. Verschillende topics hoeven niet hetzelfde aantal partities te hebben. Als een broker uitvalt, dan zal replicatie hier de situatie redden. 

\subsubsection{Replicatie}

Als er één broker uitvalt, dan is de data beschikbaar op een andere broker. Hou rekening met de hoeveelheid data dat er op een systeem wordt bijgehouden. De replicatiefactor wijst aan hoeveel keer de data moet worden bijgehouden. Data die van enorm belang zijn, krijgt best een hoge replicatiefactor toegekend, terwijl eerder dev-gerelateerde zaken zoals logging kan op een replicatiefactor van één worden gehouden.

\subsection{Data synchroon houden met een \textit{partition leader}}

Er wordt het onderscheid gemaakt tussen leaders en followers:

\subsubsection{Leaders}

Een partition leader wordt op één broker tegelijkertijd toegekend. Er kan niet meer dan één broker leider zijn van een partitie.  Verder bezit een leider twee belangrijke kenmerken.

\begin{itemize}
	\item De leider heeft volledige toegang tot de data en is de enige die data kan sturen en ontvangen. Daarmee is de leider diegene die wordt aangesproken wanneer er iets in de data moet worden aangepast.
	\item De leider kent de data-achterstand van de replica's. Met fetch requests kunnen volgers achterhalen hoeveel data ze nog moeten inhalen.
	\item De leider zal de status van \textit{out-of-sync} toekennen aan volgers die na tien seconden niets laten weten.
\end{itemize}

\subsubsection{Follower}

De andere brokers volgen de leader en worden followers genoemd. De followers hebben geen toegang tot de data, maar luisteren enkel naar de aanpassingen van de leader. Een applicatie kan, binnen dezelfde databankrack van de leader, een volger aanspreken. Dit gebeurt enkel wanneer het luisteren naar de volger resulteert in het snelst en meest correct ophalen van de data. Hiervoor moet de follower wel in-sync zijn. De volgers sturen \textit{fetch requests} naar de leader om de lokale data te updaten met de meest recente versie.

\subsubsection{Bij het uitvallen van een leader}

Wanneer een leader opeens uitvalt, dan wordt er een broker als leader van een partitie toegekend. Dit gebeurt enkel als de replica \textit{in-sync} is met de data. Er is een verschil met in-sync replica's en out-of-sync replica's.

\begin{itemize}
	\item Een \textit{in-sync} replica is een replica waarbij er geen verschil is tussen de replica en de leider. Zij bezitten de meest recente data. Enkel in-sync replica's komen in aanmerking om leider te worden.
	\item Een \textit{out-of-sync} replica beschikt niet meer over de meeste recente data. De leader bepaalt wie \textit{out-of-sync} is.
\end{itemize}

\subsection{Producer}
Een producer weet, afhankelijk van de metadata, naar welk topic en partitie zij zal sturen. Veel van de moeilijkheden worden voor de producer verborgen. 

\subsubsection{Configuratie}
Belangrijke configuratie is wanneer de producer iets ziet als geslaagd. Er zijn drie verschillende waarden:

\begin{itemize}
	\item acks=0 wijst erop dat de producer een bericht verstuurd en verder niets doet. De data verzenden is voldoende. Het is niet betrouwbaar, maar wel snel. Dit is interessant bij sensordata, maar minder interessant bij een banktransactie.
	\item acks=1 wijst erop dat de producer een bericht verstuurd én wacht op bevestiging van de broker. Dit leidt tot deelse geruststelling, want de dataverlies is beperkt. De producer geen weet op wat er na die stap gebeurt én of de data bij de \textit{in-sync replica's} is terechtgekomen.
	\item acks=all wijst erop dat alle producers wachten op de leider, en alle \textit{in-sync replicas} moeten de data ontvangen hebben. In het geval van minstens één \textit{in-sync replica} is de producer zeker dat de data in minstens twee buffers zijn opgeslaan. 
	\item min.insync.replicas=1 is een optionele parameter. Hierbij wordt er ingesteld hoeveel \textit{insync-replica's} er per broker moet zijn ingesteld.
	\begin{itemize}
		\item Replicatiefactor 3 met min-insync-replicas van 1 is gevaarlijk, want dat geeft aan dat de data verloren zijn wanneer de leader uitvalt. De producer zal denken dat het bericht is toegekomen.
		\item Replicatiefactor drie met min-insync-replicas van 2 wilt zeggen er minstens één \textit{out-of-sync} volger wordt toegelaten.
		\item Replicatiefactor 3 met min-insync-replicas van 3 is functioneel ok, maar de beschikbaarheid van het systeem zal \textit{tanken}. Er wordt geassumeerd dat er geen trage volgers zijn.
	\end{itemize}
\end{itemize}

\subsubsection{Keys}

Per boodschap wordt er een key verstuurd. Alle boodschappen met dezelfde sleutel zal naar dezelfde partitie worden verstuurd. Als er geen specifieke requirement is, dan wordt de key weggelaten. Het systeem zal de load verdelen, met behulp van een \textit{round robin} manier, over de verschillende brokers.

\subsection{Consumer}

Consumers lezen van één of meer topics. De consumers worden per partitie toegekend. Als er een fout in de cluster voordoet, dan zullen de consumers zich daar automatisch van herstellen. Data wordt gelezen in een bepaalde volgorde in een partitie. Over de partities heen is er geen garantie dat de volgorde parallel loopt of identiek is.

\subsubsection{Consumer group}

Een groep wordt bijgehouden in de vorm van een applicatie. Alle consumers in een groep krijgen dezelfde ID toegekend. Iedere applicatie, met verschillende consumers, zal lezen van partities die de applicatie werd toegewezen. De partities worden niet onderling verdeeld. Een consumer zal altijd een volledige partitie benutten. Als er een partitie bijkomt, dan kan er een derde consumer worden toegevoegd. Het is zinloos om meer consumers te hebben, dan partities. Bij vier consumers en drie partities zal één consumer geen werk kunnen verrichten. Het aantal partities is de bovengrens voor het parallelisme dat een systeem kan bereiken.

\subsubsection{Consumer offset}

Binnen Kafka is er een speciale topic, namelijk de \textit{consumer offset}. Dit is de staat van de topics per consumer. De analogie met een logboek wordt gelegd. Als de consumer alle data heeft verwerkt, dan moet de offset worden verlegd voor het lezen van de eerstvolgende offset. Het moment wanneer de offset wordt gecommit speelt een rol over hoeveel keer een bericht zal worden verwerkt. Alle gelezen berichten worden bijgehouden en zo heeft het systeem weet vanaf waar een consumer berichten mag beginnen verwerken. 

\subsection{Delivery Semantics}

Er zijn drie verschillende delivery semantics:

\begin{itemize}
	\item \textit{At most once} impliceert dat de boodschap nooit twee keer zal verwerkt worden. De kans is dat de boodschap één keer werd verwerkt, maar ook dat de consumer géén boodschap heeft verwerkt.
	\item \textit{At least once} wijst erop dat de boodschap zeker één keer werd verwerkt. Meerdere keren is mogelijk. Als de consumer uitvalt voor de commit, dan wordt de boodschap opnieuw verwerkt. Afhankelijk van de applicatie, idempotent of niet, dan kan dit worden gezien als een negatief. 
	\item \textit{Exactly once} betekent dat het bericht noch minder noch meer dan één keer mag worden gelezen.
\end{itemize}

\subsubsection{Broker Discovery}

Het maakt niet uit met welke broker er een verbinding wordt opgestart. De client zal een verbinding leggen met eender welke broker. Welke broker maakt niet uit, want de verbinding met één broker wijst aan dat er een verbinding is met de volledige cluster. Iedere broker houdt metadata bij, dus iedere broker kent de weg naar een cluster.

\subsection{Zookeeper}

Zookeeper wordt aangeraden wanneer Kafka in productie wordt gebruikt. Deze technologie wordt gebruikt om Kafka te beheren. Enkele functies van Zookeeper:

\begin{itemize}
	\item Zookeeper houdt de brokers bij.
	\item Zookeeper voert het leader-election-algoritme uit. Als er een topic online/offline wordt gehaald, dan loopt dit eerst naar Zookeeper.
\end{itemize}}

\subsubsection{Oppositie}

Zookeeper is op zich ook een gedistribueerd systeem. Nu wordt er gewerkt om Kafka zonder Zookeeper te laten draaien, maar dit werkt voorlopig enkel binnen een testomgeving.

\section{Labo}

Poortnummer 19092 wordt gebruikt. De broker maakt niet uit, want de actie zal altijd werken.

\subsubsection{Lijst tonen}

\begin{lstlisting}[language=CLI-kafka]
kafka-topics --bootstrap-server kafka1:19092 --list
kafka-topics --bootstrap-server kafka2:19093 --list
kafka-topics --bootstrap-server kafka3:19094 --list

kafka-topics --bootstrap-server 127.0.0.1:9092 --topic first_topic  --describe

kafka-topics --bootstrap-server kafka1:19092 --describe --topic lecture
\end{lstlisting}

\subsubsection{Topic aanmaken}
Opmerking: de replicatiefactor kan nooit groter zijn dan het aantal brokers.
\begin{lstlisting}[language=CLI-kafka]
kafka-topics --bootstrap-server 127.0.0.1:9092
			 --create
			 --topic first_topic
			 --partitions 3
			 --replication-factor 1
			 
kafka-topics --create --topic lecture --partitions 3 --replication-factor 3
\end{lstlisting}


\subsubsection{Topic verwijderen}

\begin{lstlisting}[language=CLI-kafka]
kafka-topics --bootstrap-server 127.0.0.1:9092
	--delete
\end{lstlisting}


\subsection{Producer}

\subsubsection{Messages aanmaken in bestaande topic}
\begin{lstlisting}[language=CLI-kafka]
kafka-console-producer --bootstrap-server kafka1:19092 
					   --topic lecture
					   --producer-property acks=all/0/1
\end{lstlisting}

\subsubsection{Messages aanmaken in nieuwe topic}

Eerst wordt er een waarschuwing gegeven, maar het topic zal worden aangemaakt met standaardparameters. Dit is standaard één replicatiefactor en één partitie, wat vaak van de benodigde parameters afwijkt. Het is beter om zelf een nieuwe topic te maken, al kunnen de standaardparameters wél in \textit{server.properties} worden ingesteld.

\subsection{Consumer}
De consumer zal enkel nieuwe berichten opvangen. Als er geen nieuwe berichten zijn, dan wordt er niets opgehaald en blijft de terminal leeg. Enkel als de optie \textit{from-beginning} wordt meegegeven, dan worden alle messages sinds het aanmaken van de topic getoond. Die volgorde is niet dezelfde als waarin de berichten werden verstuurd. De ordening van de topic is afhankelijk van de gekozen partitie.

\begin{lstlisting}[language=CLI-kafka]
kafka-console-consumer --bootstrap-server kafka1:19092
						--topic lecture
						--from-beginning
\end{lstlisting}

\subsubsection{Consumer groups}

Als een consumer hetzelfde commando tweemaal doet draaien, dan zal de uitvoer zich niet herhalen. Kafka houdt de offset bij van een consumer-group. Het tweede commando toont alle gekende consumer-groups. Met het derde commando kan er worden afgeleid hoeveel consumers er op dat moment in een groep zitten. Voor iedere partitie wordt de offset getoond van de laatste offset in de partitie en de laatste offset die werd geconsumeerd. Ofwel de log-end-offset en de current-offset.

\begin{lstlisting}[language=CLI-kafka]
kafka-console-consumer --bootstrap-server kafka1:19092
					   --topic lecture
					   --group group-lecture
					   --from-beginning
					   
kafka-consumer-groups --bootstrap-server kafka1:19092 --list

kafka-consumer-groups --bootstrap-server kafka1:19092 --describe
						--group group-lecture
\end{lstlisting}

\subsubsection{Offsets resetten}

De offset kan op vier verschillende manieren worden aangepast.

\begin{itemize}
	\item To-earliest zal de offset naar het begin verplaatsen
	\item To-datetime maakt het mogelijk om de offset te verplaatsen naar de offset die het dichtste bij een specifiek gekozen datum 
	\item Shift-by verlegt de offset met n-aantal lijnen
	\item By-period is gelijkaardig aan \textit{to-datetime}, maar hier wordt er enkel met periodes gewerkt. Bijvoorbeeld een week geleden, een dag geleden, een uur geleden enzovoort.
\end{itemize}

\begin{lstlisting}[language=CLI-kafka]
kafka-consumer-groups --bootstrap-server kafka1:19092
						--group group-lecture
						--reset-offsets --to-earliest/to-datetime/by-period/shift-by
						--execute
						--topic lecture
\end{lstlisting}

\chapter{Kafka Java Programming}

\subsubsection{Dependencies}

Het pom-bestand heeft twee dependencies nodig: kafka-clients en slf4j-simple.

\section{Producer}

\subsection{Een Kafka Producer ontwikkelen}

Een Kafka Producer in Java ontwikkelen omvat drie fasen:

\begin{enumerate}
	\item Producer-eigenschappen instellen.
	\item Producer aanmaken.
	\item Data versturen naar Kafka.
	\item Optioneel: Callback toevoegen
\end{enumerate}

\subsubsection{Eigenschappen instellen}

De drie belangrijkste eigenschappen zijn: 

\begin{itemize}
	\item bootstrap.servers dat het adres van Kafka aangeeft
	\item key.serializer
	\item value.serializer
\end{itemize}

\begin{lstlisting}[language=Java]
Properties kafkaProperties = new Properties();
String bootstrapServers = "localhost:9092";

kafkaProperties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG), bootstrapServers);
kafkaProperties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, bootstrapServers);
kafkaProperties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
\end{lstlisting}

\subsubsection{Producer aanmaken}

\begin{lstlisting}[language=Java]
KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);
\end{lstlisting}

\subsubsection{Data versturen naar Kafka}

\begin{lstlisting}[language=Java]
ProducerRecord<String, String> record = new ProducerRecord<>("topic-naam", "message-inhoud-message");
producer.send(record);
producer.flush();
producer.close();
\end{lstlisting}

\subsubsection{Callback gebruiken}

De callback wordt in de send-methode gebruikt om logs bij te houden.

\begin{lstlisting}[language=Java]
producer.send(record, new Callback(){
	@Override
	public void onCompletion(RecordMetaData recordmetadata, Exception e)
	{
		if (e == null){
			logger.info("Received new metadata"
			+ "Topic: " + recordmetadata.topic()
			+ "Partition: " + recordmetadata.partition()
			+ "Offset: " + recordmetadata.offset());	
		} else {
			logger.error("Error while producing", e);
		}
	}
})
\end{lstlisting}

\subsubsection{Keys toevoegen aan de producer}

\begin{lstlisting}[language=Java]
for (int i = 0; i < NUM_MESSAGES; i++){
	String topic = "first-topic";
	String key = "id_" + Integer.toString(i % 10);
	String message = "Hello world " + Integer.toString(i);
	
	ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, message);
}
\end{lstlisting}

\subsection{Producer Internals}

Het proces gebeurt in vier stappen:

\begin{enumerate}
	\item Bij het aanspreken van de send() methode worden de key en value geserializeerd met de serializers. Deze werden verkregen bij het aanmaken van de KafkaProducer.
	\item  Als de standaardpartitioner niet wordt gespecifieerd, dan wijst Kafka zelf een partitioner aan. De standaardpartitioner is al dan niet eerder beschreven op een \textit{round robin} manier.
	\item Topic en partitie zijn gekend. De record wordt aan de batch van records toegevoegd. Die worden dan verstuurt naar hetzelfde topic en partitie.
	\item In een aparte thread worden de batches van records naar de gepaste Kafka broker gestuurd.
	\item Wanneer de broker de berichten ontvant, dan stuurt die een antwoord terug. Er zijn twee mogelijke scenario's:
	\begin{itemize}
		\item Als de broker erin slaagt om het bericht uit te schrijven, dan wordt er een RecordMetadata-object teruggegeven. Die bevat de topic, partitie en record-offset binnen een partitie.
		\item Als de broker er niet in slaagt om een bericht uit te schrijven, dan wordt er een fout teruggegeven. De producer zal proberen om het bericht enkele keren opnieuw te sturen totdat het opgeeft en een fout teruggeeft. Het aantal keren dat er iets opnieuw wordt gestuurd is een keuze van de ontwikkelaar.
	\end{itemize}
	\item Om de producer af te sluiten wordt er gewerkt met \textit{flush} en \textit{close} methoden.
\end{enumerate}

\section{Java Consumer}

\subsection{Een Java Consumer ontwikkelen}

Een Kafka Consumer in Java ontwikkelen omvat vijf fasen:

\begin{enumerate}
	\item Properties aanmaken
	\item Een consumer aanmaken
	\item De consumer koppelen of abonneren aan één of meerdere topics.
	\item De consumer in een lus plaatsen.
	\item De consumer op een gepaste en goedgekeurde manier laten afsluiten.
\end{enumerate}

\subsubsection{Consumer properties aanmaken}

Er zijn vijf eigenschappen die een Kafka Consumer moet verkrijgen:

\begin{itemize}
	\item \textit{bootstrap.servers} ofwel de lijst van brokers waaraan de consumer zich kan aan verbinden.
	\item \textit{key.deserializer} geeft aan hoe de sleutels worden gedeserialiseerd.
	\item \textit{value.deserializer} geeft aan hoe de values worden gedeserialiseerd. In een best scenario lopen beide parallel met elkaar.
	\item \textit{group.id} is een String-object dat aangeeft aan welke consumer-group de consumer toebehoort.
	\item \textit{auto.offset.reset} bepaalt wat er moet gedaan worden als er geen offset bij aanvang is gekozen, of wanneer de huidige offset niet meer op de server bestaat. Er zijn drie mogelijkheden indien er geen offset werd gegeven:
	\begin{itemize}
		\item \textit{earliest} zal de consumer van het begin laten lezen.
		\item \textit{latest} zal de consumer vanaf de laatste offset laten lezen.
		\item \textit{none} zal een foutmelding geven als er geen offset werd teruggevonden. 
	\end{itemize}
\end{itemize}

\begin{lstlisting}[language=Java]
private static final String GROUP_ID = "lecture-app";
private static final String TOPIC = "online";

Properties properties = new Properties();
properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092"); 
properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());

properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
\end{lstlisting}

\subsubsection{Een consumer aanmaken}

\begin{lstlisting}[language=Java]
Consumer<String, String> consumer = new KafkaConsumer<>(properties);
\end{lstlisting}

\subsubsection{Een consumer koppelen of abonneren aan één of meerdere topics.}

\begin{lstlisting}
consumer.subscribe(Collections.singleton(TOPIC));
\end{lstlisting}

\subsubsection{De consumer in een lus plaatsen}

Dit is de eerste versie van hoe een Consumer in een lus wordt geplaatst. Consumers draaien vaak oneindig lang door, waardoor er hier met een oneindige loop wordt gewerkt. De poll-methode is het grote werkpaard hier, want deze methode handelt de volgende zaken af:

\begin{itemize}
	\item De \textit{poll} methode doet meer dan enkel data ophalen, daarnaast vraagt het een tijdsindicatie op. Die geeft aan hoe lang een Consumer moet wachten op nieuwe records.
	\item De poll-methode moet regelmatig gebeuren. Een conumser die geen poll doet, zal als dood worden beschouwd door de broker. 
	\item Als er records beschikbaar zijn in de consumer buffer, dan zal poll deze records onmiddellijk teruggeven. Zo niet worden ze even opgehouden.
	\item Poll geeft een lijst van records mee. Deze worden geïtereerd en iedere record wordt individueel afgehandeld.
	\item Bij de eerste poll zal er een GroupCoordinator-object worden teruggegeven. Die consumergroup wordt gejoined en uiteindelijk zal het worden toegekend aan een partitie.
	\item Een rebalance wordt behandeld in de poll-methode.
	\item Als het niet-gebruiken van een \textit{poll} langer is dan de \textit{max.poll.interval.ms}, dan wordt de consumer als dood aangezien. Dan wordt de consumer uit de consumergroep gegooid. Daarom moet de code in de loop geen onvoorspelbaar interval meemaken. Als het verwerken te lang duurt, dan kan dit onopgemerkte problemen veroorzaken. Hou extra berekeningen buiten de while-lus. 
\end{itemize}

\begin{lstlisting}[language=Java]
while(true){
	ConsumerRecords<String,String> records = consumer.poll(Duration.ofMillis(100));
	
	for(ConsumerRecord<String,String> record:records){
	logger.info("key: " + record.key() = ", value: " + record.value());
	logger.info("Partition: " + record.partition() + ", Offset:" + record.offset());
	}
}
\end{lstlisting}

\section{Java Threads}

Een Java-programma kan uit meerdere execution threads bestaan. Iedere thread heeft een eigen \textit{method-call stack} en een eigen \textit{program counter}. Een thread kan tegelijkertijd met andere threads worden uitgevoerd. Daarnaast kunnen threads ook resources met elkaar uitwisselen. Dit maakt het programmeren moeilijk, want de toegang tot de gedeelde middelen moet correct worden opgevolgd.

\subsubsection{Voorbeelden van threads}

Er worden twee voorbeelden aangehaald:

\begin{itemize}
	\item Een Kafka producer maakt een aparte thread aan om record-batches naar de correcte Kafka-brokers door te sturen.
	\item Een Kafka consumer stuurt \textit{heart-beats} in een achtergrondservice.
\end{itemize}}

\subsubsection{Shutdown hook}

De Java Virtual Machine of JVM laat het uitvoeren van \textit{shutdown hooks} toe vooraleer een applicatie wordt afgesloten. Deze functies zijn ideaal wanneer er middelen worden afgenomen. Een shutdown hook is een aangemaakte, maar nog niet begonnen, thread. In Eclipse werkt dit niet, want in Eclipse is het klikken op de rode stop-knop geen normale \textit{shutdown} van het JVM. Dit kan enkel worden uitgetest in de Vagrant-machine door met Ctrl+C te werken.

\begin{enumerate}
	\item Als de JVM begint met de applicatie af te sluiten, dan worden alle geregistreerde hooks in een willekeurige volgorde genoteerd.
	\item Na het uitvoeren van alle hooks zal de JVM 
	stoppen.
\end{enumerate}

\begin{lstlisting}[language=Java]
public class ShutdownHookExample {
	public static void main(String[] args){
		System.out.println("The name of the main thread is: " + Thread.currentThread().getName());
		
		Runtime.getRuntime().addShutdownHook(new Thread() {
			@Override
			public void run(){
			System.out.println("Shutdownhook is executing in thread: " + Thread.currentThread().getName());
			}
		}
	}
}
\end{lstlisting}

\subsubsection{Atomic Boolean en het sluiten van de Consumer}

\begin{itemize}
	\item Het aanmaken van een AtomicBoolean buiten de main-methode.
	\item Het gebruiken van de AtomicBoolean binnen de main-methode. Een consumer moet op normale wijze gesloten worden. Dit gebeurt met de \textit{close}-methode. Hiervan kan er ook een oneindige poll-lus van worden gemaakt.
\end{itemize}

\begin{lstlisting}[language=Java]
final AtomicBoolean stopRequested = new AtomicBoolean(false);
final Thread mainThread = Thread.currentThread();

Runtime.getRuntime().addShutdownHook(new Thread() {
	@Override
	public void run() {
		stopRequested.set(true);
		
		try {
			mainThread.join();
		} catch (InterruptedException e) {
			e.printStackTrace();
		} finally {
			System.out.println("Done waiting for main thread");
		}
	}
});
\end{lstlisting}

\begin{lstlisting}[language=Java]
try {
	while (!stopRequested.get()) {
		ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(500));
		
		for (ConsumerRecord<String, String> record : records) {
			System.out.println("Received record with key: " + record.key() + "and value" + record.value());
		}
	}
} finally {
	System.out.println("About to close the consumer.");
	consumer.close();
	System.out.println("Consumer closed");
}
\end{lstlisting}

\section{Commits en offsets}

Kafka laat consumers toe om het systeem te laten gebruiken als een tracker per partitie. 

\begin{itemize}
	\item Kafka zelf zal geen records committen, want de consumers committen een laatste bericht dat ze zelf hebben kunnen verwerken. Er wordt simpelweg vanuit gegaan dat ieder bericht, voor het laatste, ook werd verwerkt. 
	\item Een consumer zal de offset committen wanneer er een bericht naar Kafka wordt gestuurd, die zal namelijk de \textit{\_consumer\_offsets} topic gaan updaten. Merk op dat de gecommitte offset diegen is van het volgende bericht dat de consumer wilt ontvangen.
\end{itemize}}

\subsection{Verloren berichten en dubbels}

Er zijn verschillende maatregelen om dit tegen te gaan.

\begin{itemize}
	\item Kafka slaat alle berichten in een \textit{retention period} op. Zo krijgen consumers de kans om de berichten op hun eigen tempo op te halen. Bij een stroomstoring of dergelijke kan de consumer weer oppikken waar ze waren gebleven.
	\item Consumer groups laten consumers toe om van eenzelfde topic parallel te lezen. Iedere consumer in een consumergroep wordt toegekend aan een unieke verzameling van partities. Ieder bericht wordt enkel naar één consumer in de consumergroep gestuurd, om zo load balancing en fouttolerantie te voorzien in een consumergroup.
	\item Kafka laat producers toe om een \textit{required acknowledgement level} aan ieder bericht toe te voegen. Dit belet hoeveel kopieën van het bericht naar Kafka moet worden geschreven vooraleer de producer \textit{acknowledgement} krijgt.
\end{itemize}

\begin{figure}
	\includegraphics[width=\linewidth]{images/Screenshot_266.png}
	\caption{Bovenaan wordt, door de commit, een bericht gemist door de consumer. Onderaan haalt een consumer een bericht dubbel op.}
\end{figure}

\subsection{API's om de offset te committen}

Er zijn drie soorten commits:

\begin{itemize}
	\item Automatisch is wanneer de \textit{enable.auto.commit} optie op true staat.
	\item Synchroon en asynchroon worden gebruikt wanneer de \textit{enable.auto.commit} op false staat.
\end{itemize}

\subsubsection{Automatische offset commit}

De consumer zal iedere laatste offset van een poll elke vijf seconden, per standaard, committen naar de Kafka broker. De offset wordt teruggegeven in de laatste poll, als de consumer op tijd checkt. Het interval wordt aangepast met \textit{auto.commit.interval.ms} en dit wordt geactiveerd met \textit{enable.auto.commit=true}. Opmerking: Alle berichten moeten verwerkt zijn wanneer de poll werd opgeroepen. Bij het aanspreken van de \textit{close}-methode wordt de offset ook gecommit.

\begin{figure}
	\includegraphics[width=\linewidth]{images/kafka-automatic-commit.png}
	\caption{Automatische offsets commits}
\end{figure}

\subsubsection{Synchrone offset commits}

Offsets worden enkel gecommit als de applicatie het expliciet zegt. 

\begin{itemize}
	\item Bij een \textit{commitSync} zal de laatste offset teruggegeven worden met de poll-methode en wanneer de offset wordt gecommit.
	\item Deze vorm zal blijven proberen om te committen voor zolang er geen fout is waar er niet van hersteld kan worden. Als de commit faalt, dan wordt er een foutmelding gegooid.
	\item Een \textit{commitSync} zal de laatste offset van \textit{poll} teruggeven. 
	\begin{itemize}
		\item Als er wordt gecommit vooraleer de laatste berichten werden verwerkt, dan loopt het systeem het risico dat de gecommitte berichten niet verwerkt konden worden.
		\item Als de commit na het verwerken van de berichten plaatsvindt, dan loopt het systeem het risico dat berichten dubbel worden verwerkt.
	\end{itemize}
\end{itemize}

\begin{lstlisting}[language=Java]
properties.setProperty("enable.auto.commit","false");

try
{
	while(!stopRequested.get())
	{
		ConsumerRecords<String,String> records = consumer.poll(...);
		try 
		{
			consumer.commitSync();
		} catch(CommitFailedException e) {LOGGER.error("commit failed", e);}
	}
} 
	finally 
{
	try
	{
		consumer.commitSync();
	} catch (CommitFailedException e){ LOGGER.error("commit failed");}
	consumer.close();
}
\end{lstlisting}

\subsubsection{Asynchrone commit}

Bij een synchrone commit wordt de applicatie geblokkeerd totdat de broker antwoordt op de commit-request. In tegenstelling tot een asynchrone commit API zal de consumer een commit-request sturen en gewoon doorwerken. De consumer werkt gewoon door, omdat de volgende commit mogelijks al succesvol is. Er wordt niets opnieuwe geprobeerd, want mogelijks wordt de vorige offset overschreven door een latere offset. De code volgt een gelijkaardige lijn als die van de synchrone commit.

\subsubsection{Het combineren van een synchrone en een asynchrone commit}

Een asynchrone commit is snel, maar het is niet geweten wanneer die succesvol is. Het doet er weining toe, want de volgende commit zal eigenlijk dienen als een \textit{retry}. Als de consumer wilt sluiten, dan is er geen 'volgende commit', dus er wordt als sluitende commit een synchrone commit gebruikt met \textit{close()}.

\begin{lstlisting}[language=Java]
properties.setProperty("enable.auto.commit","false");

try
{
	while(!stopRequest.get())
	{
		ConsumerRecords<String,String> records = consumer.poll(...);
		consumer.commitAsync();
	}
} 
	finally 
{
	try 
	{
		consumer.commitSync();
	} catch (CommitFailedException e) {LOGGER.error("commit failed",e)}
		{
			consumer.close();
		}
}
\end{lstlisting}

\section{Apache Avro}

Enkel String-objecten werden tot nu toe naar Kafka gestuurd. De data (de)serializeren gebeurde respectievelijk met StringSerializer en StringDeserializer. Het is niet aangeraden om een eigen Serializer of Deserializer te schrijven. Veel toepassingen verwachten dat gestructureerde objecten in en uit Kafka kunnen worden utigewisseld. 

\subsection{Avro}

Avro is een taalonafhankelijk, serialisatieformaat dat schema's opstelt. 

\begin{itemize}
	\item Deze schema's worden vaak in JSON opgemaakt en dient om binaire bestanden te gaan serializeren. Ze zijn zelfomschrijvend waardoor de consumer, zonder toegang tot de initialisatiecode, het schema kan begrijpen.
	\item Avro gaat er vanuit dat er een schema aanwezig is wanneer bestanden worden gelezen en geschreven. Dit gebeurt vaak door het schema in bestanden zelf te gaan embedden, onder de vorm van metadata.
	\item Avro ondersteund verschillende soorten datatypen en bezit een goede ondersteuning voor \textit{backward \& forward compatibility}. Dit is ideaal voor systemen waarin data tussen meerdere systemen worden uitgewisseld.
	\item Avro moet aan de POM-file worden toegevoegd, zowel bij dependencies als bij plugin.
\end{itemize}

\subsubsection{Voorbeeld Avro schema}

Hieronder wordt er een skeletstructuur getoond. Dit schema houdt een type 'User' bij die drie velden heeft: \textit{favourite number}, \textit{favourite colour} en een \textit{name}. Enkel het naamveld is verplicht, want de volgende twee waarden kunnen ook een null-waarde aannemen. 

\begin{itemize}
	\item Op basis van dit schema wordt er Java-code aangemaakt, maar het werkpaard hier is de Avro-compiler. De klassenaam zal hier \textit{be.hogent.dit.tin.avro.User} noemen.
\end{itemize}

\begin{lstlisting}[language=JSON]
{
	"type": "record",
	"name": "User",
	"namespace": "be.hogent.dit.tin",
	"fields": [
	{ "name": "name", "type": "string" },
	{ "name": "favorite_number",  "type": ["int", "null"] },
	{ "name": "favorite_color", "type": ["string", "null"] }
	]
}
\end{lstlisting}}

\begin{lstlisting}[language=JSON]
{
	"name": "Alice",
	"favorite_number": 7,
	"favorite_color": "red"
}
\end{lstlisting}

\subsection{Een klasse aanmaken met Avro}

Er zijn drie manieren:

\begin{itemize}
	\item Constructor aanspreken met meerdere methoden.
	\item Alle eigenschappen in de constructor aanspreken.
	\item Aanmaken met de builder.
\end{itemize}

\begin{lstlisting}[language=Java]
User user1 = new User();
user1.setName("Alice");
user1.setFavoriteNumber(7);
user1.setColor("red");

User user2 = new User("Dylan", 25, "violet");

User user3 = User.newBuilder().setName("Stijn")
								.setFavouriteColor(null)
								.setFavouriteNumber(314)
								.build();
\end{lstlisting}

\subsection{De klasse serializeren}

De drie gebruikers worden vervolgens op deze manier naar de disk geserializeerd.

\begin{enumerate}
	\item Alleereerst wordt er een DatumWriter-object aangemaakt. Die zal de Java-objecten omzetten naar een in-memory serialized format. De SpecificDatumWriter-klasse wordt gebruikt bij aangemaakte klassen, zodat het schema later kan worden opgehaald.
	\item Vervolgens wordt er een DataFileWriter aangemaakt. Die zal de geserializeerde records en het schema naar een specifiek bestand uitschrijven.
	\item Met append worden de gebruikers in een bestand geschreven.
	\item Het dataFileWriter-object wordt, eenmaal klaar, gesloten.
\end{enumerate}

\begin{lstlisting}[language=Java]
DatumWriter<User> userDatumWriter = new SpecificDatumWriter<>(User.class);
DataFileWriter<User> dataFileWriter = new DataFileWriter<>(userDatumWriter);

dataFileWriter.create(user1.getSchema(), new File("users.avro"));
dataFileWriter.append(user1);
dataFileWriter.append(user2);
dataFileWriter.append(user3);
dataFileWriter.close();
\end{lstlisting}

\subsection{Een klasse deserializeren.}

\begin{enumerate}
	\item Eerst wordt er een DatumReader-object aangemaakt. In dit geval een SpecificDatumReader dat in-memory geserializeerde items zal omzetten naar instanties van de aangemaakte klassen. In dit geval is de klasse User.
	\item De DatumReader en het bestand dat moet gelezen worden, wordt doorgegeven aan de DataFileReader. Dit object zal zowel het schema als de data van de disk lezen.
	\item Itereer door de gebruikers in het bestand.
	\item Het User-object wordt opnieuw gebruikt om zo zuinig mogelijk deze applicatie af te werken. Op deze manier hebben we minder \textit{object-allocation}. 
\end{enumerate}

\begin{lstlisting}[language=Java]
File file = new File("users.avro");
DatumReader<User> userDatumReader = new SpecificDatumReader<>(User.class);
DataFileReader<User> dataFileReader = new DataFileReader<>(file, userDatumReader);
User user = null;
while(dataFileReader.hasNext())
{
	user = dataFileReader.next(user);
	System.out.println(user);
}
dataFileReader.close();
\end{lstlisting}

\subsubsection{Het aantal gebruikte schema's}

Er worden twee schema's gebruikt. Het schema voor de writer is in het bestand en is nodig om te weten in welke volgorde de gebruikers werden uitgeschreven. Daarnaast is er een reader schema dat aangeeft welke velden er in een bestand zitten. De twee schema's verschillen op de manier hoe ze worden opgelost.

\subsection{Een klasse deserializeren zonder het aangemaakte User-object}

Data in Avro wordt altij bij het bijhorende schema opgeslaan. Daarom kan een geserializeerde item altijd worden gelezen, zonder dat het schema vooraf is gekend. Dit laat ons toe om zowel serializatie als deserializatie uit te voeren zonder het aangemaakte User-object. Een herwerking van het vorige voorbeeld ziet er als volgt uit:

\subsubsection{Het aanmaken van de Users}

\begin{enumerate}
	\item Eerst wordt er een Parser-object gebruikt om het schema in te lezen en een Schema-object aan te maken.
	\item Er is geen weet van hoe het schema eruitziet, dus er wordt gewerkt met GenericRecords. Dit object gebruikt het schema om te bevestigen of alles wel geldig is.
	\item Als er een niet-bestaand veld wordt ingegeven, zoals "favourite\_animal", dan wordt er een \textit{AvroRuntimeException} teruggegeven in de compiler.
\end{enumerate}

\begin{lstlisting}[language=Java]
public class AvroVoorbeeldZonderUserObject 
{
	private static final String SCHEMA_FILE_PATH = "src/main/resources/avro/Users.avsc";
	
	public static void main(String[] args) throws IOException
	{
		Schema schema = new Schema.Parser().parse(
			new File(SCHEMA_FILE_PATH)
		);
		
		GenericRecord user1 = new GenericData.Record(schema);
		user1.put("name","Alice");
		user1.put("favorite_number", 7);
		user1.put("favorite_color", "red");
		
		GenericRecord user2 = new GenericData.Record(schema);
		user2.put("name","Stijn");
	}
}
\end{lstlisting}

\subsubsection{Gebruikers uitschrijven}

Gebruikers uitschrijven gelijkt zeer hard op de vorige uitwerking, maar enkel hier wordt er weer gewerkt met een generieke versie van het oorspronkelijke object. 

\begin{itemize}
	\item Het schema wordt gebruikt om te achterhalen hoe de GenericRecord moet worden geschreven. 
	\item Daarnaast wordt het schema ook gebruikt om alle non-nullable velden te detecteren.
\end{itemize}

\begin{lstlisting}[language=Java]
File file = new File("users.avro");
DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(schema);
DataFileWriter <GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);
dataFileWriter.create(schema, file);
dataFileWriter.append(user1);
dataFileWriter.append(user2);
dataFileWriter.close();
\end{lstlisting}

\subsubsection{Gebruikers inlezen}

De gouden regel geld ook bij het inlezen. Alle objecten worden aangepast naar een generieke versie van datzelfde object.

\begin{lstlisting}[language=Java]
DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema);
DataFileReader<GenericRecord> dataFileReader = new DataFileReader<>(file,datumReader);
GenericRecord user = null;
while(dataFileReader.hasNext())
{
	user = dataFileReader.next(user);
	System.out.println(user);
}
\end{lstlisting}

\subsubsection{Het uitgeschreven bestand}

De applicatie geeft de volgende output terug.

\begin{lstlisting}[language=JSON]
{”name”: ”Alyssa”, ”favorite_number”: 256, ”favorite_color”: null}{”name”: ”Ben”, ”favorite_number”: 7, ”favorite_color”: ”red”}
\end{lstlisting}

Het uitgeschreven bestand is niet met het blote oog begrijpbaar. Daarvoor moeten we met \textit{format-hex} het bestand bekijken. Daar merken we op dat het schema helemaal bovenaan het bestand zichtbaar is. Het schema maakt deel uit van het uitgeschreven bestand. Dit is OK wanneer het bestand groot is, want in dat geval bevat het bestand veel records ofwel veel data. De berichten in Kafka zijn echter van beperkte grootte, dus het schema in ieder bericht plaatsen zou veel overhead veroorzaken.

\section{Het schema-register}

Het volledige schema in ieder bericht opslaan zou leiden tot veel overhead. Weglaten kan ook niet, want Avro heeft het schema nodig om een record te kunnen lezen. De oplossing is om het schema elders op te slaan.

\subsection{Werking van een schema-register}

Bij alle Kafka-records worden enkel het ID van het schema opgeslaan. De consumers halen een record op en vragen dan ook aan het register welk schema er nodig is om een record te deserializern. De serializer en deserializer staat in voor het opslaan en ophalen van het schema. De consumers en brokers halen niets direct op vanuit het schema register.

\begin{figure}
	\includegraphics[width=\linewidth]{images/serializer-schema.png}
	\caption{Werking van een schema-register.}
\end{figure}

\subsection{Kafka-producer met Avro}

In de volgende code worden er drie nieuwe zaken aan het properties-bestand toegevoegd. Merk op dat er voor de volgende code extra dependencies en repositories moeten worden toegevoegd in de POM-file. 

\begin{itemize}
	\item Er wordt een String als sleutel gebruikt, maar toch wordt er gevraagd aan Avro om de sleutel te serializeren. De twee config-klassen worden gebruikt om de keys en values van de berichten te serializeren.
	\item Daarnaast moet er ook een url richting het schema-register worden toegevoegd.
\end{itemize}

\newpage

\begin{lstlisting}[language=Java]
public class KafkaAvroProducerExample {
	
	private static final String TOPIC = "users";
	private static final String BOOTSTRAP_SERVER = "127.0.0.1:9092";
	private static final String SCHEMA_REGISTRY_URL = "http://127.0.0.1:8081";

	public static void main(String[] args) {
		
		// Create the KafkaProducer
		Properties props = new Properties();
		props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER);
		
		// Use Avro to serialize the key, even though we will use Strings as key
		props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
		io.confluent.kafka.serializers.KafkaAvroSerializer.class.getName());
		// Use Avro to serialize the value. 
		props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
		io.confluent.kafka.serializers.KafkaAvroSerializer.class.getName());
		
		props.put("schema.registry.url", SCHEMA_REGISTRY_URL);
		
		// Use the generated class as the value class for the KafkaProducer.
		Producer<String, be.hogent.dit.tin.avro.User> producer = new KafkaProducer<>(props);
		
		// Create some User objects and put them in a list.
		// This is identical to the code in AvroExample.
		List<User> users = new ArrayList<>();
		
		User user1 = new User();
		user1.setName("Alyssa");
		user1.setFavoriteNumber(256);
		// Leave favorite color null
		users.add(user1);
		
		// Alternate constructor
		User user2 = new User("Ben", 7, "red");
		users.add(user2);
		
		// Construct via builder
		User user3 = User.newBuilder()
		.setName("Charlie")
		.setFavoriteColor("blue")
		.setFavoriteNumber(null)
		.build();
		users.add(user3);
		
		// Send the records to Kafka. Note how this exactly the same as before.
		// We use the name of the user as the key.
		for (User user : users) {
			ProducerRecord<String, User> record = 
			new ProducerRecord<>(TOPIC, user.getName().toString(), user);			
			producer.send(record);
			System.out.println("User " + user + " sent");
		}
		
		
		// flush and close the producer
		producer.flush();
		producer.close();	
	}	
}
\end{lstlisting}

\newpage

\subsection{Kafka Consumer met Avro}

\begin{lstlisting}[language=Java]
public class KafkaAvroConsumerExample {
	
	private static final String TOPIC = "users";
	private static final String BOOTSTRAP_SERVER = "127.0.0.1:9092";
	private static final String SCHEMA_REGISTRY_URL = "http://127.0.0.1:8081";
	private static final String GROUP_ID = "my-avro-app";
	
	public static void main(String [] args) {
		
		// Create the KafkaProducer
		Properties props = new Properties();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER);
		props.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
		
		// Use String deserializer for the key
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		
		// Use KafkaAvroDeserializer for the value
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
		io.confluent.kafka.serializers.KafkaAvroDeserializer.class.getName());
		
		props.put("schema.registry.url", SCHEMA_REGISTRY_URL);
		props.put("specific.avro.reader", "true");
		
		// Add the shutdown hook for proper shutdown of consumer
		final Thread mainThread = Thread.currentThread();
		final AtomicBoolean stopRequested = new AtomicBoolean(false);
		
		Runtime.getRuntime().addShutdownHook(new Thread() {
			public void run() {
				System.err.println("Starting exit .....");
				stopRequested.set(true);
				try {
					mainThread.join();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			}
		});
		
		// Create the consumer and subscribe to topics
		Consumer<String, User> consumer = new KafkaConsumer<>(props);
		consumer.subscribe(Collections.singleton(TOPIC));
		
		try {
			while (!stopRequested.get()) {
				ConsumerRecords<String, User> records = consumer.poll(Duration.ofMillis(500));
				
				for (ConsumerRecord<String, User> record : records) {
					System.out.println("Received record with key [" + record.key() + "]");
					System.out.println("Value of this record is " + record.value());
					System.out.println("The type of the value is " + record.value().getClass().getName());
				}
			}
		} finally {
			System.out.println("Starting to close the consumer");
			consumer.close();
			System.out.println("Consumer closed");
		}
	}	
}
\end{lstlisting}

\section{Voorbeelden}

\subsection{ConsumerOffsetExample1}

\begin{lstlisting}[language=Java]
public class ConsumerOffsetExample1 {
	
	private static final Logger LOGGER = LoggerFactory.getLogger(ConsumerOffsetExample1.class);
	
	public static void main(String[] args) {
		
		String bootstrapServers = "localhost:9092";
		String groupId = "my-application";
		String topic = "first_topic";
		
		Properties properties = new Properties();
		
		properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
		properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
			StringDeserializer.class.getName());
		properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
			StringDeserializer.class.getName());
		properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId);
		properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
		
		// Each call to poll will return at most 2 records
		properties.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "2"); 
		
		// Create consumer
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
		
		// Subscribe to topic
		consumer.subscribe(Collections.singleton(topic));
		
		// Add the shutdown hook
		final Thread mainThread = Thread.currentThread();
		final AtomicBoolean stopRequested = new AtomicBoolean(false);
		
		Runtime.getRuntime().addShutdownHook(new Thread() {
			public void run() {
				System.err.println("Starting exit .....");
				stopRequested.set(true);
				try {
					mainThread.join();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			}
		});
		// Start the (infinite) poll loop. Surround with try-catch to catch the WakeupException
		try {
			while (!stopRequested.get()) {
				ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
				
				for (ConsumerRecord<String, String> record : records) {
					LOGGER.info("key: " + record.key() + ", value: " + record.value());
					LOGGER.info("Partition: " + record.partition() + ", Offset:" + record.offset());
				}
				
				try {
					Thread.sleep(2000); // Sleep for two seconds 
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			}
		} finally {
			System.err.println("Starting to close the consumer");
			consumer.close();
			System.err.println("Consumer closed");
		}		
	}
}
\end{lstlisting}

\subsection{ConsumerOffsetExample2}

\begin{lstlisting}[language=Java]
public class ConsumerOffsetExample2 {
	private static final Logger LOGGER = LoggerFactory.getLogger(ConsumerDemo.class);
	public static void main(String[] args) {
		
		String bootstrapServers = "localhost:9092";
		String groupId = "my-application";
		String topic = "first_topic";
		
		Properties properties = new Properties();
		
		properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
		properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId);
		properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
		
		
		// Disable automatic commit
		properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
		// Each call to poll will return at most 2 records
		properties.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "2"); 
		
		// Create consumer
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
		
		// Subscribe to topic
		consumer.subscribe(Collections.singleton(topic));
		
		// Add the shutdown hook
		final Thread mainThread = Thread.currentThread();
		final AtomicBoolean stopRequested = new AtomicBoolean(false);
		LOGGER.info("mainThread has name: " + mainThread.getName());
		
		Runtime.getRuntime().addShutdownHook(new Thread() {
			public void run() {
				System.err.println("Starting exit .....");
				LOGGER.info("shutDownHook running in thread: " + Thread.currentThread().getName());
				stopRequested.set(true);
				try {
					mainThread.join();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			}
		});
		
		// Start the (infinite) poll loop. Surround with try-catch to catch the WakeupException
		try {
			while (!stopRequested.get()) {
				ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
				
				for (ConsumerRecord<String, String> record : records) {
					LOGGER.info("key: " + record.key() + ", value: " + record.value());
					LOGGER.info("Partition: " + record.partition() + ", Offset:" + record.offset());
				}
				
				try {
					Thread.sleep(2000); // Sleep for two seconds 
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
				
				// Commit the offsets
				try {
					consumer.commitSync();
				} catch (CommitFailedException e) {
					LOGGER.error("Commit failed", e);	
				}
			}
		} finally {
			System.err.println("Starting to close the consumer");
			// Commit the offsets just before closing the consumer
			try {
				consumer.commitSync();
			} catch (CommitFailedException e) {
				LOGGER.error("Commit failed", e);	
			}
			consumer.close();
			System.err.println("Consumer closed");
		}	
	}	
}
\end{lstlisting}

\subsection{ConsumerOffsetExample3}

\begin{lstlisting}[language=Java]
public class ConsumerOffsetExample3 {
	
	private static final Logger LOGGER = LoggerFactory.getLogger(ConsumerDemo.class);
	
	
	public static void main(String[] args) {
		
		String bootstrapServers = "localhost:9092";
		String groupId = "my-application";
		String topic = "first_topic";
		
		Properties properties = new Properties();
		
		properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
		properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId);
		properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
		
		
		// Disable automatic commit
		properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
		// Each call to poll will return at most 2 records
		properties.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "2"); 
		
		// Create consumer
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
		
		// Subscribe to topic
		consumer.subscribe(Collections.singleton(topic));
		
		// Add the shutdown hook
		final Thread mainThread = Thread.currentThread();
		final AtomicBoolean stopRequested = new AtomicBoolean(false);
		LOGGER.info("mainThread has name: " + mainThread.getName());
		
		Runtime.getRuntime().addShutdownHook(new Thread() {
			public void run() {
				System.err.println("Starting exit .....");
				LOGGER.info("shutDownHook running in thread: " + Thread.currentThread().getName());
				stopRequested.set(true);
				try {
					mainThread.join();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			}
		});
		
		
		// Start the (infinite) poll loop. Surround with try-catch to catch the WakeupException
		try {
			while (!stopRequested.get()) {
				ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
				
				for (ConsumerRecord<String, String> record : records) {
					LOGGER.info("key: " + record.key() + ", value: " + record.value());
					LOGGER.info("Partition: " + record.partition() + ", Offset:" + record.offset());
				}
				
				try {
					Thread.sleep(2000); // Sleep for two seconds 
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
				
				// Commit the offsets asynchronously				
				consumer.commitAsync();				
			}
		} finally {
			System.err.println("Starting to close the consumer");
			// Commit the offsets synchronously just before closing the consumer
			try {
				consumer.commitSync();
			} catch (CommitFailedException e) {
				LOGGER.error("Commit failed", e);	
			}
			consumer.close();
			System.err.println("Consumer closed");
		}
		
	}
}
\end{lstlisting}

\section{Oefeningen}

\subsection{KafkaProducer aanmaken}

\begin{lstlisting}[language=Java]
public class ProduceLogMessages {
	
	static final String BOOTSTRAP_SERVER = "localhost:9092";
	static final String TOPIC = "log.messages";
	static final Integer DELAY_MS = 1000; // 1000 ms
	
	public static void main(String[] args) {
		
		Properties properties = new Properties();
		
		final AtomicBoolean stopRequested = new AtomicBoolean(false);
		final Thread mainThread = Thread.currentThread();
		Runtime.getRuntime().addShutdownHook(new Thread() {
			@Override
			public void run() {
				System.out.println("Shutdownhook called");
				stopRequested.set(true);
				try {
					mainThread.join();
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		});
		
		// Bootstrap-server instellen
		properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER);
		
		//Key & Value serializer
		properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		
		final Producer<String, String> producer = new KafkaProducer<String, String>(properties);
		final LogMessageGenerator generator = new LogMessageGenerator();
		
		try {
			while (!stopRequested.get()) {
				String msg = generator.next(); // volgende line offset in een bestand
				String[] parts = msg.split(" "); // message opsplitsen in een array
				String sourceSystem = parts[1]; // want de key is het tweede element in de string
				ProducerRecord<String, String> record = 
							new ProducerRecord<String, String>(TOPIC, sourceSystem, msg);
				producer.send(record); 
				try {
					Thread.sleep(DELAY_MS);
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		} finally {
			System.out.println("Closing the producer...");
			producer.close();
			System.out.println("Closed the producer");
		}
	}	
}
\end{lstlisting}

\newpage
\subsection{Consumer / Producer}
\begin{lstlisting}[language=Java]
public class ConsumeLogMessages {
	static final String BOOTSTRAP_SERVER = "localhost:9092";
	static final String TOPIC = "log.messages";
	static final String GROUP_ID = "important";
	
	static final Integer DELAY_MS = 1000; // 1000 ms
	
	public static void main(String[] args) {
		
		String GROUP_ID = ""; 
		
		// create properties for consumer and producer
		Properties properties = new Properties();
		properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER);
		properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		
		properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
		properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
		
		
		// create ConsumerRunnable
		final ConsumerRunnable consumer = null;
		final Thread consumerThread = new Thread(consumer);
		consumerThread.start();
		
		
		// Wordt aangesproken als het programma opeens wordt afgesloten.
		Runtime.getRuntime().addShutdownHook(new Thread() {
			@Override
			public void run() {
				System.out.println("Shutdownhook called. Calling shutdown");
				consumer.shutdown();
				
				try {
					consumerThread.join(); // wacht tot de andere thread klaar is
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
				System.out.println("Done waiting on consumer");
			}
		});
	}
}
\end{lstlisting}
\newpage
\subsubsection{ConsumerRunnable}
\begin{lstlisting}[language=Java]
class ConsumerRunnable implements Runnable {
	
	private final Consumer<String, String> consumer;
	private final Producer<String, String> producer;
	private final AtomicBoolean stopRequested;
	
	public ConsumerRunnable(Properties consumerProperties, Properties producerProperties) {
		this.consumer = new KafkaConsumer<String, String>(consumerProperties);
		this.producer = new KafkaProducer<String, String>(producerProperties);
		this.stopRequested = new AtomicBoolean(false);
	}
	
	public void run() {
		// hier komt de while-loop
		try{
			while(!stopRequested.get()) {
				ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(500));
				for(ConsumerRecord<String, String> record : records) {
					System.out.println("Received record with key: " + record.key() + "and value" + record.value());
				}
			}
		} finally {
			System.out.println("About to close the consumer.");
			consumer.close();
			System.out.println("Consumer closed");
		}
	}
	
	public void shutdown() {
		stopRequested.set(true);
	}
}
\end{lstlisting}

\chapter{Spark}

\subsubsection{Vooraf}

Voor de komst waren er een aantal programmeermodellen gericht op filesystemclusters. De meeste waren gespecialiseerd, waaronder MapReduce, Storm, Impala en Pregel. Spark werd ontworpen om sneller en flexibeler te zijn dan de vooraf genoemde modellen. Sindsdien is het een populaire keze geworden voor een heleboel dataverwerkingstaken, waaronder \textit{batch processing}, streamverwerking, \textit{machine learning} en gegevensanalyse. Het biedt een aantal voordelen t.o.v. andere clustercomputersystemen, zoals: 

\begin{itemize}
	\item Flexibele en expressieve API
	\item Ondersteuning voor een breed aantal gegevensbronnen en formaten
	\item Mogelijk om te draaien in een groot aantal deployment-omgevingen, zoals on-premises clusters, cloudgebaseerde clusters en zelfs op één enkele machine.
\end{itemize}

\subsubsection{Algemene voordelen}

Spark is generiek en algemeen. Een analoge uitleg is om Spark een smartphone te noemen, terwijl andere modellen eerder een GPS, mobiele telefoon of digitale camera zijn. Spark is een \textit{jack-of-all-trades}. daarom brengt het drie voordelen met zich mee:

\begin{itemize}
	\item Toepassingen zijn makkelijker te ontwikkelen, want ze maken gebruik van een gecentraliseerde API.
	\item Het is efficiënter om verwerkingstaken te combineren. Voor Spark moeten pipelines vaak de gegevens wegschrijven naar opslag om ze door te geven aan een andere engine. Spark kan diverse functies uitvoeren op dezelfde gegevens, vaak in het geheugen.
	\item Spark maakt nieuwe toepassingen mogelijk, bijvoorbeeld interactieve queries op het streamen van ML.
\end{itemize}

\section{Spark Application Architecture}

Er zijn drie belangrijke rollen:

\begin{itemize}
	\item Bij een Spark-applicatie is de \textit{driverprocess} hetgeen wat de applicatiecode doet draaien. Dit proces onderhoudt de informatie van de Spark-applicatie, beantwoordt de input van de gebruiker en verdeeld het werk over de \textit{executors}.
	\item Een executorprocess gaat het werk uitvoeren dat hen werd toegekend. De staat van de actie (geslaagd/niet geslaagd) wordt gerapporteerd aan het driverproces.
	\item De \textit{clustermanager} houdt logs bij van de beschikbare resources en welke resources er aan de executors, indien mogelijk, kunnen worden gegeven. Spark ondersteunt verschillende clustermanagers, waaronder YARN, Mesos en Kubernetes.
\end{itemize}


\begin{figure}
	\begin{center}
		\includegraphics[height=5cm]{images/Screenshot_258.png}
	\end{center}
	\caption{Opbouw en structuur van een Spark-applicatie}
\end{figure}


\subsubsection{Partitionering}

Parallelle verwerking met Spark kan enkel gebeuren wanneer het systeem in verschillende clusters is opgebouwd. Een partitie is in dit geval een verzameling van rijen die op een fysieke machine in een cluster is opgeslaan. De partities van een DataFrame tonen hoe de data fysiek is verdeeld over de cluster heen. Het aantal partities is gebonden op het niveau van parallelisme in een Spark applicatie. Bij één partitie zal de parallellisatiefactor gelijk staan aan één, zelfs al zijn er meerdere executors. Omgekeerd ook, want als er meerdere partities zijn met één executor, dan zal de parallellisatiefactor gelijk staan aan één. De afweging maken tussen aantal partities en aantal executors speelt hier een rol op het vlak van snelheid.

\section{Spark in Java}

\subsection{Dataframe}

Een dataframe is een gedistribueerde, in-memory tabel met benoemde kolommen en een schema. Het schema duidt elke specifiek datatype aan. Een dataframe is \textit{immutable}, dus eenmaal aangemaakt kan het object niet meer worden gewijzigd. Dataframes kunnen worden aangemaakt uit verschillende bronnen, zoals Hadoop InputFormats of CSV-bestanden. Verdere transformaties, zoals aggregaties en filteren, kunnen zeker toegepast worden op eender welk type formaat. In Java-code verwijzen we naar het object 'Dataset<>'. Tussen de diamandtekens plaatsen we het object 'Row'.

\subsection{Transformaties}

Er zijn verschillende types transformaties die op een dataframe kunnen worden uitgevoerd. Deze transformaties verlopen \textit{lazy evaluated}. Het onderscheid tussen \textit{wide} en \textit{narrow} transformaties wordt hier gemaakt:

\begin{itemize}
	\item Een narrow transformatie is wanneer het resultaat voor een partitie kan worden berekend met enkel de data van die partitie. Iedere inputpartitie contribueert tot minstens één outputpartite. Deze transformatie kan ook gebeuren aan de hand van een pipeline, zo hoeft er geen data naar de disk te worden geschreven. Narrow transformaties zijn snel en efficiënt.
	\item Een wide transformatie heeft nood aan meerdere outputpartities. Sommige inputpartities zullen contribueren tot elk meerdere outputpartities. Hier is er nood aan een \textit{shuffle}, waarbij er een proces wordt toegevoegd. In dit proces worden de partities, over een cluster heen, onderling met elkaar uitgewisseld. Een shuffle vereist dat er resultaten naar de \textit{disk} wordt geschreven. Aangezien dit extra schrijfoperaties zijn, is het aangeraden om het aantal wide-transformaties zo minimaal mogelijk te houden.
\end{itemize}

\subsubsection{Lazy Evaluation}

Spark maakt gebruik van Lazy evaluation, wat wilt zeggen dat een transformatie niet zal resulteren in een nieuwe dataframe. De return-waarde is wél een nieuw object dat het resultaat van de bewerking bijhoudt. De berekening is nog niet uitgevoerd. Bij het triggeren van een actie kijkt Spark naar de graph van alle transformaties die werden opgeroepen. Net zoals bij SQL wordt er een \textit{query execution plan} aangemaakt. Dit proces noemt \textit{query optimization}. Spark kan mogelijks meerdere filters en filters gaan samensteken, of het kan andere soorten optimizatie gaan toepassen om zo min mogelijk data te moeten verwerken.

\subsection{Acties}

Transformaties in Spark bouwen een logische transformatieplan op. Die geeft aan hoe de inputdata moet worden getransformeerd om de gewenste output te krijgen. Het transformatieplan wordt niet uitgevoerd tot de actie wordt getriggerd. Een actie geeft Spark de instructie om een resultaat of verzameling van transformaties te berekenen. Sommige voorbeelden van acties zijn:

\begin{itemize}
	\item Sommige rijen van een dataframe tonen.
	\item Het aantal rijen van een dataframe optellen.
	\item De dataframe opslaan als een native object, zoals een Java lijst.
	\item Een dataframe schrijven naar een externe \textit{data sink}.
\end{itemize}

Eenmaal de actie is getriggerd, dan zal Spark het transformatieplan uitvoeren en de resultaat van de actie berekenen. 

\subsection{Physical plan}
Spark's Physical Plan wijst aan hoe de Spark-applicatie zal uitgevoerd worden op een cluster van machines. Dit omvat details zoals:
\begin{itemize}
	\item Welke transformaties moeten er worden uitgevoerd?
	\item Hoe moet de data gepartitioneerd of geshuffled worden?
	\item Hoe moeten de resultaten van de transformaties worden opgeslaan of geretourneerd?
\end{itemize}

\subsubsection{Het belang van een physical plan}

Het \textit{physical plan} is belangrijk, want zo achterhalen ontwikkelaars hoe zij hun middelengebruik kunnen optimaliseren en het aantal data, dat tussen machines heen moet worden geschreven, kan geminimaliseerd worden. Verdere oplossingen zijn:

\begin{itemize}
	\item Debugging en troubleshooten.
	\item Performance optimization
	\item Resource management
	\item Capaciteitsplanning
\end{itemize}

\section{Spark Libraries}

Dit is puur informatief. Spark bevat meerdere libraries om data-analyse of data-berekeningen op uit te voeren.

\begin{itemize}
	\item SQL en dataframe
	\item Spark Streaming
	\item MLLib
	\item GraphX
\end{itemize}

\subsection{Integratie met opslagsystemen}

Spark is ontworpen om met verschillende externe systemen te werken. Het is \textit{storage-system agnostic}, wat wilt zeggen dat het met een breed gamma aan storage systemen kan werken. Enkele voorbeelden van Spark-geïntegreerde storage systemen zijn:

\begin{itemize}
	\item HDFS is de populairste keuze voor Big Data, zowel gestructureerde als non-gestructureerde data.
	\item Key-value stores
	\item Relationele databanken
\end{itemize} 

\subsection{Resilient Distributed Dataset}

Voor Apache 2.0 werd er gewerkt met een Resilient Distributed Dataset of RDD object. Dit is een verzameling van objecten die parallel gemanipuleerd kunnen worden. Het is een fouttolerante werkwijze. RDD's zijn ook immutable en ze worden aangemaakt wanneer er wordt gelezen van een databron. RDD's zijn meer in de achtergrond verdwenen bij de nieuwere versies, maar ze blijven toegankelijk. RDD's komen in deze cursus niet meer aan bod.

\section{Spark-applicaties schrijven}

\subsection{Een dataframe met nieuwe waarden aanmaken.}

Stap voor stap wordt de volgende code uitgelegd:

\begin{enumerate}
	\item Eerst wordt er een spark-object aangemaakt in de main-methode. Zonder deze kan de applicatie niet werken.
	\item Er wordt een nieuwe lege lijst aangemaakt. We houden een Java-lijst bij van "Row's", ofwel de inhoud voor de dataframe.
	\item We maken vijf nieuwe rijen aan. In iedere rij houden we twee waarden bij: tekst en een getal. Met de RowFactory wordt er een Row-object aangemaakt, om dan vervolgens aan de Java-lijst toe te voegen.
	\item De komende twee stappen zijn optioneel, maar aan te raden. De ambitie is om een schema op te stellen. Het schema is een weergave van alle datatypes die in de dataframe aan bod komen. Zonder een schema moet Spark zelf uitvissen wat de datatypes zijn, en zelfs dan is er géén garantie dat het de juiste datatype is. In dit geval zijn er twee velden: naam in de vorm van een String en leeftijd in de vorm van een Integer. Deze lijst houdt de StructFields of schemavelden bij die in een schema moeten voorkomen. Dit is het schema nog niet!
	\item Op basis van de lijst met datatypes wordt er een schema gemaakt. Het schema is van het type StructType.
	\item Het dataframe wordt aangemaakt. Hiervoor komt het Spark-object van pas. Samen met de lijst van rijen als eerste parameter én het schema wordt het dataframe aangemaakt.
	\item Vervolgens wordt de data gegroepeerd op basis van naam. Zo zijn er twee Brooke's in het dataframe. In het resultaat zal er maar één Brooke te vinden zijn, maar van de twee leeftijden wordt het gemiddelde genomen.
	\item Het dataframe wordt in de terminal weergegeven. Alle kolommen, dus naam en leeftijd, worden weergegeven.
	\item Optioneel, maar wel aangeraden om het Spark-object te sluiten.
\end{enumerate}

\begin{lstlisting}[language=Java]
	public	static void main(String[] args){
		
		SparkSession spark = SparkSession.builder()
		.appName("applicatiePersonenMetLeeftijd")
		.master("local[*]")
		.getOrCreate();
		
		List<Row> inMemory = new ArrayList<>();
		
		inMemory.add(RowFactory.create("Brooke",20));
		inMemory.add(RowFactory.create("Brooke",25));
		inMemory.add(RowFactory.create("Denny",31));
		inMemor.add(RowFactory.create("Jules",30));
		inMemory.add(RowFactory.create("TD",35));
		
		List<StructField> fields = Arrays.asList(
		DataTypes.createStructField("name",DataTypes.StringType, true),
		DataTypes.createStructField("age",DataTypes.IntegerType, true);
		)
		
		StructType schema = DataTypes.createStructType(fields);
		
		Dataset<Row> df = spark.createDataFrame(inMemory, schema);
		
		Dataset<Row> result = df.groupby("name").avg("age");
		
		result.show();
		
		spark.close();
	}
\end{lstlisting}

\subsection{Een bestaande dataframe lezen}

Een dataframe wordt vaak gelezen uit een externe bron, zoals JSON, CSV, Parquet of Kafka output. Het Spark-object bezit een methode \textit{read()}. Deze methode geeft een DataFrameReader-object terug, wat nodig is om een bron in te lezen. Veronderstel dat we hier met dezelfde dataset te maken hebben, maar enkel al vooraf opgeslaan in een csv-bestand.

\begin{enumerate}
	\item Spark-object aanmaken.
	\item Schema maken door een lijst van StructFields te maken én die dan te gieten in een StructType-object.
	\item Het CSV-bestand wordt ingelezen. Net zoals het aanmaken, gebeurt het inlezen ook met het Spark-object. In het CSV-bestand is er een header, vandaar dat er eerst de optie 'header:true' wordt meegeven. Vervolgens wordt het schema meegegeven én uiteindelijk het pad waar de CSV kan worden teruggevonden.
	\item Net zoals daarnet wordt er gegroepeerd op naam. Per naam wordt de gemiddelde leeftijd teruggegeven.
	\item Het dataframe wordt in de terminal getoond.
\end{enumerate}

\begin{lstlisting}[language=Java]
	public static void main(String[] args){
		
		SparkSession spark = SparkSession.builder()
		.appName("applicatiePersonenMetLeeftijd")
		.master("local[*]")
		.getOrCreate();
		
		List<StructField> fields = Arrays.asList(
		DataTypes.createStructField("name",DataTypes.StringType, true),
		DataTypes.createStructField("age",DataTypes.IntegerType, true);
		)
		
		StructType schema = DataTypes.createStructType(fields);
		
		Dataset<Row> df = spark.read()
		.option("header", true)
		.schema(schema)
		.csv("src/main/resources/PersonenMetLeeftijd.csv");
		
		Dataset<Row> result = df.groupby("name").avg("age");
		
		result.show();
		
		spark.close();
	}
\end{lstlisting}

\subsubsection{Schema overerven}

In de vorige twee berekeningen werd er een schema zelf, door de ontwikkelaar, aangemaakt. Dit is vrijblijvend, want Spark kan ook een schema overerven. Hieronder is er een alternatieve methode op stap twee van het vorige voorbeeld. Deze methode is minder efficiënt, want Spark moet zelf achterhalen wat de meest geschikte datatypes zijn. Daarnaast gaat Spark lijn per lijn door de volledige dataset heen. De workload kan worden geminimaliseerd door het schema op te bouwen o.b.v. een deel van de volledige dataset. Dit is een deelse oplossing, want er kunnen nog steeds problemen voorkomen bij de gekozen datatypes. Daarom is het aangeraden om zelf een schema op te bouwen.

\begin{lstlisting}[language=Java]
	Dataset<Row> df  = spark.read()
	.option("header", true)
	.option("inferSchema", true)
	.option("samplingRation", 0.0001)
	.csv("src/main/resources/PersonenMetLeeftijd.csv)
\end{lstlisting}

\subsection{Een dataframe uitschrijven naar een nieuw bestand.}

In de volgende code wordt er verondersteld dat er een Spark-object is aangemaakt, een schema is opgesteld én een dataset al is ingelezen. De dataset werd opgeslaan als DataFrame onder de naam 'result'.

\begin{enumerate}
	\item Het pad wordt tweemaal gebruikt, dus dat wordt opgeslaan onder een variabele. Het pad kan zowel relatief als absoluut zijn. In dit voorbeeld is het pad absoluut.
	\item Er zijn drie opties:
	\begin{enumerate}
		\item De inhoud van de dataframe wordt uitgeschreven naar een parquet-bestand. Dit bestand is een must bij een column-oriented databankstructuur.
		\item Vervolgens wordt de mode meegegeven. Bij het uitschrijven wordt er een map gegenereerd. In die map kan het parquet-bestand worden teruggevonden.
		\item Als laatste moet er een outputpad worden meegegeven. 
	\end{enumerate}
	\item Puur uit controle wordt het net aangemaakte bestand ingelezen door Spark. De outputpad, waar het parquet-bestand werd opgeslaan, dient nu als inputpad.
	\item Het Spark-object wordt gesloten.
\end{enumerate}

\begin{lstlisting}[language=Java]
	String path = "file:///C:/tmp/personen";
	
	df.write()
	.format("parquet")
	.mode("overwrite")
	.save(path);
	
	Dataset<Row> dfParquet = spark.read().parquet(path);
	
	spark.close();
\end{lstlisting}

\section{Dataframe-kolommen inlezen, toevoegen en verwijderen.}

\subsubsection{Kolommen toevoegen}
Een dataframe bestaat uit kolommen. Sommige functies vragen de naam van een kolom op als String, maar andere functies vereisen dat er een object van de klasse 'Column' wordt meegegeven. In het volgende voorbeeld wordt er een extra kolom aangemaakt. De kolom is een berekening op de leeftijdskolom. 

\begin{enumerate}
	\item Bovenaan komen de imports. De klasse 'functions' is met een kleine letter geschreven, daarmee een uitzondering op de regel om alle klassen met een hoofdletter te laten starten.
	\item Er worden twee kolommen aan de dataset toegevoegd. Een expressie of where-clausule wordt meegegeven met de \textit{expr(...)}-functie.
\end{enumerate}

\begin{lstlisting}[language=Java]
	import org.apache.spark.sql.Column;
	import static org.apache.spark.sql.functions.col;
	
	df = df.withColumn("ageNextYear2", expr("age + 1"))
	.withColumn("over30", expr("age > 30"));
\end{lstlisting}

\subsubsection{Kolommen selecteren}
Tot nu toe werden alle kolommen van de dataframes bij de keuze betrokken. Spark laat de ontwikkelaar toe om kolommen, net zoals bij SQL, te kiezen. Hieronder wordt er een voorbeeld gegeven van de verschillende kolommen. Er wordt verondersteld dat er al een spark-object is aangemaakt én dat het pad in een String 'inputpath' wordt bijgehouden.

\begin{enumerate}
	\item Er wordt een nieuwe dataset opgehaald. De dataset is opgeslaan in een CSV-bestand. Het bevat vijf kolommen: id, score, naam, jaartal en kwartaal. Enkel bij dit voorbeeld wordt het schema overgeërfd. 
	\item De dataset wordt in de terminal getoond. Alle vijf kolommen worden weergegeven.
	\item Met de \textit{select}-functie worden enkel de score, jaartal en kwartaalkolommen opgehaald. Een dataframe is \textit{immutable}, dus het object moet worden overschreven.
	\item De dataset bestaat nu uit drie kolommen. 'Id' en 'naam' zijn niet meer terug te vinden in de dataset.
\end{enumerate}

\begin{lstlisting}[language=Java]
	Dataset<Row> df = spark.read()
	.option("header",true)
	.option("inferSchema",true)
	.option("sampleRation", 0.001)
	.csv(inputpath);
	
	df.show();
	
	df = df.select("score","jaartal","kwartaal");
	
	df.show();
\end{lstlisting}

\subsubsection{Kolommen verwijderen}

Een kolom kan op een passieve wijze worden weggelaten door de kolom niet in een \textit{select} te betrekken. In sommige gevallen, bijvoorbeeld bij een groot aantal kolommen, is het handiger om één specifieke kolom te verwijderen, in plaats van een select uit te voeren. De gouden regel is dat dataframes immutable zijn, dus hier moet de nieuwe versie van de dataframe opnieuw wordt toegekend.

\begin{lstlisting}
	df = df.drop("id","naam");
\end{lstlisting}

\subsubsection{Kolommen hernoemen}

Kolomnamen kunnen worden aangepast. Bij het toevoegen wordt er vaak een standaardnaam toegekend aan een kolom, bijvoorbeeld \textit{count} of \textit{sum}. Hieronder wordt de kolom met de naam \textit{count} aangepast naar \textit{aantalRijen}. Na een berekening wordt er ook de \textit{alias}-functie gebruikt. 

\begin{lstlisting}[language=Java]
	df = df.withColumnRenamed("count","aantalRijen");
	df = df.count().alias("aantalRijen");
\end{lstlisting}

\subsection{Dataframes filteren}

\subsubsection{Voorbeelden van de studentendataset}

\begin{enumerate}
	\item Hou een dataset bij met alle rijen die een score hebben van 'A+'.
	\item Hou een dataset bij met alle rijen waarvan de score 'B' is én de score werd behaald in het jaar 2010 of 2011.
	\item Neem de dataset van nummer 3 en behoud enkel de unieke rijen.
\end{enumerate}

\begin{lstlisting}[language=Java]
	Dataset<Row> dfAplus = df.select()
	.where(col("grade")
	.equalTo("A+");
	
	Dataset<Row> dfB1011 = df.select()
	.where(col("grade").equalTo("B")
	.and(col("year").isin(2010,2011)));
	
	Dataset<Row> dfUniekB1011 = dfB1011.distinct();
\end{lstlisting}

\subsection{Aggregaties uitvoeren op dataframes}

\subsubsection{Voorbeelden van de studenten-dataset}

\begin{lstlisting}[language=Java]
	Dataset<Row> dfCountPerJaar = df.select("year")
	.groupby("year")
	.count()
	.orderBy(desc("count"));
	
	Dataset<Row> dfStatisticsPerJaarEnOnderwerp = df.select("year","subject","score")
	.groupby("year","subject")
	.agg(max("score"), min("score"), avg("score"))
	
\end{lstlisting}

\subsubsection{Voorbeelden van de ViewingFigures-oefening}

\begin{lstlisting}[language=Java]
	Dataset<Row> chaptersPerCourse = chaptersDF
	.drop("chapterId")
	.groupBy("courseId").count()
	.withColumnRenamed("count", "chapters");
\end{lstlisting}


\subsection{User Defined Functions}

User Defined Functions of UDF's worden ingezet wanneer een berekening de ingebouwde functies van Spark overstijgt. De berekening maken is mogelijk, maar de logica is te complex en vereist daarmee een aparte aanpak. Bij een UDF zijn er drie stappen:

\begin{enumerate}
	\item UDF schrijven.
	\item UDF registreren.
	\item UDF oproepen.
\end{enumerate}

\begin{lstlisting}[language=Java]
	UDF1<Double, Integer> score = new UDF1<Double, Integer>() {
		public Integer call(Double percent) throws Exception {
			if (percent > 0.9) {
				return 10;
			} else if (percent > 0.5) {
				return 6;
			} else if (percent > 0.25) {
				return 2;
			} else {
				return 0;
			}
		}
	};
	
	spark.udf().register("berekenScore", score, DataTypes.IntegerType);
	
	
	Dataset<Row> result = percentageDF
	.withColumn("score",  call_udf("berekenScore",col("percentage")))
	.drop("percentage")
	.groupBy("courseId").agg(sum("score").as("total"))
	.join(titleDF, "courseId")
	.orderBy(desc("total"));
\end{lstlisting}

\newpage

\section{Oefeningen op Spark}

\subsection{Inverted Index}

\begin{lstlisting}[language=Java]
public class InvertedIndex {
	
	static SparkSession spark;
	
	static final String INPUT_PATH = "src/main/resources/*";
	
	static UDF1<String, String> parseFilename = new UDF1<String, String>()
	{
		public String call(String fullPath) throws Exception {
			int index = fullPath.lastIndexOf("/");
			return fullPath.substring(index + 1, fullPath.length());
		};
	};
	
	public static void main(String[] args) {
		
		spark = SparkSession.builder()
							.appName(InvertedIndex.class.getName())
							.master("local[1]")
							.getOrCreate();
							
		spark.sparkContext().setLogLevel("ERROR");
		
		Dataset<Row> invertedIndex = spark.read()
			.text(INPUT_PATH);
		
		spark.udf()
			.register("parseFilename", parseFilename, DataTypes.StringType);
		
		invertedIndex
			.withColumn("raw", input_file_name().cast("String")) 			
			.withColumn("filename", call_udf("parseFilename", col("raw"))) 	
			.withColumn("woorden", explode(split(col("value"), " ")))		
			.drop("raw")													
			.groupBy(col("woorden"), col("filename"))						
			.agg(count("woorden").as("aantalVoorkomens"))
			.orderBy(desc("aantalVoorkomens"))
			.show();
		
		
		// extra aanvulling: ervoor zorgen dat de bestanden waarin spark terechtkomt opnemen
		invertedIndex
			.withColumn("raw", input_file_name().cast("String")) 			
			.withColumn("filename", call_udf("parseFilename", col("raw"))) 	
			.withColumn("woorden", explode(split(col("value"), " ")))  		
			.withColumn("woorden", lower(col("woorden")))
			.drop("raw")													
			.groupBy(col("woorden"))										
			.agg(count("woorden").as("aantalVoorkomens"), 					
					collect_set(col("filename")).as("bestandenVoorkomens"))	
			.orderBy(desc("aantalVoorkomens"))
			.show(false);	
	}
}
\end{lstlisting}

\newpage

\subsection{Fire Department}
\begin{lstlisting}[language=Java]
public class SanFransiscoFireDepartment {
		
	static SparkSession spark;
	static final String PATH = "src/main/resources/output";
		
	public static void main(String[] args) {
			
		spark = SparkSession.builder()
				.appName("InvertedIndex")
				.master("local[*]")
				.getOrCreate();
			
		spark.sparkContext().setLogLevel("ERROR");
			
		List<StructField> fields = Arrays.asList(DataTypes.createStructField("CallNumber", DataTypes.IntegerType, true),
		DataTypes.createStructField("UnitID", DataTypes.StringType, true),
		...
		DataTypes.createStructField("Delay", DataTypes.DoubleType, true));
			
		StructType schema = DataTypes.createStructType(fields);
			
		// Data inlezen
		Dataset<Row> department = spark.read()
			.option("header", true)
			.schema(schema)
			.csv("src/main/resources/sf-fire-calls.csv");
			
		// Part I
			
		// convert the columns calldate and watchdate
		department = department
			.withColumn("Calldate", to_date(col("CallDate"), "MM/dd/yyyy"))
			.withColumn("Watchdate", to_date(col("Watchdate"), "MM/dd/yyyy"));
			
		// convert the column availabledtm to a timestamp
		department = department
			.withColumn("AvailableDtTm", 
				to_timestamp(col("AvailableDtTm"), "MM/dd/yyyy hh:mm:ss a"));
			
		// save the dataframe as a parquet file
		department.write()
			.mode(SaveMode.Overwrite)
			.parquet(PATH);
						
		// Part II
		// Alle verschillende calltypes in 2018 met WHERE
		department
			.where(year(col("Calldate")).equalTo("2018"))
			.groupBy(col("CallType"))
			.agg(count("*").as("Aantal"))
			.select("CallType")
			.orderBy("CallType")
			.show(false);
			
		// Alle verschillende calltypes in 2018 met filter
		department
			.filter(year(col("Calldate")).equalTo("2018"))
			.groupBy(col("CallType"))
			.agg(count("*").as("Aantal"))
			.select("CallType")
			.orderBy("CallType")
			.show(false);
\end{lstlisting}
		
		\newpage
		
		\begin{lstlisting}[language=Java]
			
		// Alle maanden in 2018 met het hoogst aantal calls.
		department
			.filter(year(col("Calldate")).equalTo("2018"))
			.groupBy(month(col("Calldate")).as("Maandnummer"))
			.agg(count("*").as("Aantal"))
			.orderBy(desc("Aantal"))
			.show(false);
			
		// De neighborhood met de meeste oproepen in 2018
		department
			.filter(year(col("Calldate")).equalTo("2018"))
			.groupBy("neighborhood")
			.agg(count("*").as("Aantal"))
			.orderBy(desc("Aantal"))
			.limit(1)
			.show(false);
			
		// De neighborhoods met de slechtste gemiddelde vertraagtijden bij een omroep.
		department
			.filter(year(col("Calldate")).equalTo("2018"))
			.groupBy("neighborhood")
			.agg(avg(col("Delay")).as("avgResponse"))
			.orderBy(desc("avgResponse"))
			.select(col("neighborhood"), bround(col("avgResponse"), 2))
			.show(false);
			
		// De week in 2018 met de meeste calls
		department
			.filter(year(col("Calldate")).equalTo("2018"))
			.groupBy(weekofyear(col("Calldate")).as("Weeknummer"))
			.agg(count("*").as("Aantal"))
			.limit(1)
			.show(false);
			
			spark.close();
		}
	}
\end{lstlisting}

\chapter{Spark Streaming}

\section{Stream Processing}

\subsubsection{Structured Spark batch processing}

Voordien gebeurde alles bij Spark in één keer. Dit wordt ook batch processing genoemd en omvat de volgende drie kenmerken:

\begin{itemize}
	\item De data is bounded. De analogie wordt gemaakt met de logistiek bij de Colruyt. Wanneer de winkel dichtgaat, dan wordt er een verslag gemaakt van alles wat die dag werd verkocht. Bij een CSV of XML-bestand is de laatste rij gekend.
	\item Er wordt géén data toegevoegd. Het lezen van data gebeurt in één batch. Werken met batches is geen ideale methode voor real-time dataverwerking.
	\item Alle data wordt in een tabel bijgehouden.
\end{itemize}

\subsection{Spark Stream Processing}

Data wordt continu aan het systeem toegevoegd en moet in real-time worden verwerkt. Bijvoorbeeld bij een live dashboard. De status van de data is hier belangrijk. De data is niet-gebonden of \textit{unbounded}, wat wilt zeggen dat er continu rijen achteraan worden toegevoegd. Het probleem is dat er géén oneindige tabel in het geheugen is. Een stream bestaat uit drie kenmerken:

\begin{itemize}
	\item Het is \textit{unbounded}, al denkt het systeem niet zo. Er is géén vast einde van de inputdata gekend en de stroom vloeit op een niet-gekende manier. Piekperioden, rustige momenten en alles tussenin zijn mogelijke scenario's.
	\item Er wordt telkens data achteraan de inputdata toegevoegd. Iedere nieuwe record in een datastream wordt behandeld als een nieuwe rij. De rij wordt telkens aan de inputtabel achteraan toegevoegd. Het real-time aspect klinkt als een radicale verandering voor het programmeren, maar alle code wordt geschreven zoals ze voor een batch processing job werd geschreven. Deze job moet telkens getriggerd worden om data te verkrijgen. Pas bij de trigger kijkt Spark voor nieuwe data en die data wordt achteraan de inputdata toegevoegd.
	\item Alle data wordt in een tabel bijgehouden.
\end{itemize}


\subsubsection{Data ophalen met sockets}

Spark zal gemiste input vergeten, want er wordt gewerkt met sockets. Een socket leest enkel de binnenkomende info. Data die voordien werd gestuurd, zal niet worden gelezen. Historiek wordt niet bijgehouden. Als de deur toe is, dan zal niemand staan wachten tot de deur opengaat. 

\subsubsection{Output sink}

Dit is de bestemming waarnaar de data wordt geschreven. Dit kan een databank, bestand of systeem zijn dat data kan verwerken of manipuleren. Bij stream processing is dit het systeem dat de resultaten van real-time verwerkingsoperaties zal bijhouden. Een output sink dient als werkpaard om de gestreamede data persistent bij te houden.

\subsection{Outputmodes}

Er zijn drie outputmodes: append, update en complete. Deze instelling geeft aan hoe de data tijdelijk moet worden bijgehouden.

\begin{itemize}
	\item Appendmode zal enkel de nieuwe rijen als resultaat zien. De historiek of bestaande rijen worden niet getoond of aangepast. Dit is de standaardmodus. Met enkel de nieuwe rijen is er weinig mogelijk. Zo is WordCount niet odnersteudn, want het aantal voorkomens wordt niet lang bijgehouden én de vorige geschreven data wordt niet betrokken. Dit is de meest efficiënte outputmode.
	\item Updatemode zal zowel nieuwe als oude rijen teruggeven. Van de oude rijen worden enkel aanpassingen gemaakt. Als een woord bij de vorige trigger werd gegeven, en nu zijn er vijf voorkomens meer, dan wordt dat aantal opgeteld. 
	\begin{itemize}
		\item Sommige outputsinks ondersteunen deze modus niet, want de bestanden hebben geen interne structuur. Eenmaal een bestand geschreven is, dan is het moeilijk om dit aan te passen. Bijvoorbeeld als de vorige rij vijf kolommen had, en nu wordt er een rij toegevoegd met zes kolommen, dan zal dit resulteren in een fout.
	\end{itemize}
	\item Completemode zal het volledige resultaat uitschrijven. De output zal stelselmatig groeien en de job zal in de tussentijd niet draaien. Dit is een \textit{go-to} mode voor aggregatieberekeningen, maar wordt met voorkeur gekozen voor kleine datasets. WordCount werkt wel bij deze modus. Deze mode is de meest flexibele outputmode.
\end{itemize}

\subsubsection{Voorbeeld stock prices}

Stel dat er een continue flow van stock prices is. Bij appendmode worden enkel de binnenkomende shares getoond. Bij updatemode worden enkel de veranderingen, dus nieuwe en aangepaste shares, weergegeven. Bij completemode worden alle shares na iedere trigger uitgeprint.

\section{Code}

Een streamingquery opbouwen vereist vijf stappen:

\begin{enumerate}
	\item Inputsources bepalen
	\item Data omzetten
	\item Outputsink en outputmode bepalen
	\item Processingsdetails bepalen
	\item Query starten
\end{enumerate}}


\subsection{Inputbron bepalen}

Nu wordt er gewerkt met een \textit{readStream} methode. Bij batch-processing werd er gewerkt met een gewone \textit{read}-methode. De keuzes bij stream-processing lopen in lijn met batch-processing. 
\begin{itemize}
\item De format zal in de meeste gevallen een socket zijn, wat voordien een CSV, parquet of tekstbestand was. 
\item Bij de opties worden de netwerkinstellingen en Kafka-informatie meegegeven. Het poort van de socket, gegevens over de broker, etc.
\item Load start het leesproces.
\end{itemize}

\begin{lstlisting}[language=Java]
Dataset<Row> df = spark.readStream()
.format("socket")             
.option("host", "localhost")  
.option("port", 9999)         
.load();
\end{lstlisting}

\subsection{Data omzetten}

Je hebt enkel de huidige data nodig. Data kunnen zich in twee statussen bevinden: stateful en stateless. De opdeling loopt in lijn met \textit{wide} en \textit{narrow} berekeningen.

\begin{itemize}
\item Bij stateful kan iedere rij op zich worden verwerkt. Met enkel de huidige informatie is het systeem voldoende. Bijvoorbeeld mapping volgens select of filters.
\item Bij stateless is de informatie van voordien nodig. Stateless komt bij een aggregatie goed van pas. De meeste combinaties worden ondersteund, tenzij de combinaties te moeilijk zijn om op een incrementele manier te berekenen.
\end{itemize}

\begin{lstlisting}[language=Java]
Dataset<Row> words = lines.select(
explode(split(col("value"), "\\s")).as("word")
);

Dataset<Row> counts = words.groupBy("word").count();
\end{lstlisting}

\subsection{Outputsink -en modus bepalen}

Er wordt bepaald hoe de geschreven outputdata moet worden geschreven. De outputmode en de outputlocatie moeten in de code te zien zijn. De code hieronder bouwt verder op de de counts-dataset die net werd opgesteld:
\begin{itemize}
\item In de code hieronder wordt er gewerkt met een complete outputmode, want er wordt een aggregatie op de dataset gemaakt.
\item Alle output wordt naar de console geschreven. Andere mogelijkheden zijn: 'memory', 'kafka', 'rate', 'parquet' of 'csv'. 
\end{itemize}

\begin{lstlisting}[language=Java]
DataStreamWriter<Row> writer = counts.writeStream()
.format("console")
.outputMode(OutputMode.Complete());
\end{lstlisting}

\subsection{Processingdetails bepalen}

De laatste stap voor het starten van de query is het specifiëren hoe de data moet worden verwerkt. Er zijn vier mogelijkheden:

\begin{itemize}
\item De standaardoptie is het behandelen in microbatches. Deze worden kort na elkaar uitgevoerd. Wel moet de onderlinge tijd worden meegegeven. 
\item Werken met een vast triggerinterval is soortgelijk aan Linux cronjobs. Op een vooraf gespecifieerd moment wordt de query uitgevoerd.
\item Alle data wordt op één moment verwerkt, daarna stopt de applicatie. Alle nieuwe data wordt in één batch verwerkt. Dit is handig wanneer er controle over een externe scheduler moet worden genomen.
\item Continuous is een experimentele modus. Hierbij de data zo snel als mogelijk continu verwerkt, dus niet meer in micro-batches. Enkel een klein aantal dataframebewerkingen ondersteunen deze modus. De latency is hier minimaal. 
\end{itemize}}

\subsubsection{Code}

Dit is een aanvulling op de code van stap 3. Onder format en outputmode wordt er een trigger meegegeven. In dit geval gebeurt de verwerking iedere seconde. 

\begin{lstlisting}[language=Java]
DataStreamWriter<Row> writer = counts.writeStream()
.format("console")
.outputMode(OutputMode.Complete())
.trigger(Trigger.ProcessingTime(1,TimeUnit.SECONDS));
\end{lstlisting}

\subsection{Query starten}

De query wordt gestart met een start-methode op de DataStreamWriter. Dit geeft een StreamingQuery terug, wat de actieve query voorstelt. Een StreamingQuery kan dienen voor verder beheer van de query.

\begin{lstlisting}[language=Java]
StreamingQuery streamingQuery = writer.start();
try{
streamingQuery.awaitTermination();
} except {
e.printStackTrace();
};
\end{lstlisting}

\section{Sources \& Sinks}

\subsection{Bestanden lezen}

Bestanden in een directory kunnen worden ingezet als input voor een stream. Wel moeten er drie zaken in het achterhoofd worden gehouden:

\begin{itemize}
\item Alle bestanden moeten in eenzelfde formaat zijn geschreven. Alles is ofwel een CSV-formaat, ofwel een tekstformaat, ofwel een specifiek ander formaat.
\item Bestanden moeten atomisch beschikbaar zijn. Tijdens het lezen mogen de bestanden niet worden bewerkt of verwijderd.
\item Bestanden worden genomen op basis van de timestamp, maar het verwerken daarentegen is niet vooraf bepaald. Alle bestanden worden parallel gelezen. Er is geen garantie welk bestand er eerst wordt bekeken.
\item Naar een bestand schrijven gebeurt in append-only mode. Het is makkelijker om bestanden aan een bestaande directory toe te voegen, dan de inhoud van bestaande bestanden aan te passen.
\end{itemize}


\subsection{Kafka-inhoud lezen}

Net zoals bij bestanden kan de inhoud van een Kafka topic worden gezien als een stream. Het schema bij Kafka Dataframes zal altijd hetzelfde zijn. Parsen zal wel altijd noodzakelijk zijn.

\begin{itemize}
\item De key-value wordt als binary teruggegeven, maar dit is in binary. 
\item De topic wordt als een String teruggegeven.
\item De partitie waar de record in terug kan worden gevonden.
\item De line offsset value van een record.
\item De timestamp van de record en het type van de timestamp.
\end{itemize} 

\subsubsection{Naar Kafka schrijven.}

Het is mogelijk om in alle drie outputmodes naar Kafka uit te schrijven. Ter aanvulling is het ook mogelijk om van een topic naar een topic te schrijven.Completemode wordt wel niet aangeraden. Bij het uitschrijven naar Kafka moet er met het volgende schema worden gewerkt:

\begin{itemize}
\item De key is optioneel. Dit in de vorm van een String of een binaire waarde.
\item De value is verplicht. 
\item de topic is verplicht wanneer er geen topic werd meegegeven als optie. Als dit werd meegegeven in de applicatie, dan wordt deze kolom in het schema genegeerd.
\end{itemize}

\begin{lstlisting}[language=Java]
SparkSession spark = SparkSession.builder();

final String topic = args[0];

Dataset<Row> messages = spark.readStream()
.format("kafka")
.option("kafka.bootstrap.servers"), BOOTSTRAP_SERVERS)
.option("subscribe", topic)
.load();

StreamingQuery query = null;

try {
query = messages.writeStream()
.format("console")
.outputMode(OutputMode.append())
.option("checkpointLocation", CHECKPOINT_LOCATION)
.start();
} catch {
e.printStackTrace();
}

try {
query.awaitTermination();
} catch (StreamingQueryException e) {
e.printStackTrace();
}

spark.close();
\end{lstlisting}

\section{Aggregations with Event-Time windows}

Gebruikelijk worden aggregaties niet over een volledige stream gedaan, maar in kleine stukken over een tijdspanne. Bijvoorbeeld in een casus waar sensoren data iedere minuut doorsturen. Hier ligt de nadruk eerder op wanneer de data werd aangemaakt en niet op wanneer de data werd verwerkt door Spark. Iedere timestamp wordt naar een window gemapt. Zo wordt er een groupby op het interval uitgevoerd.

\subsubsection{Tumbling windows}

Tumbling windows zijn windows van een vaste grootte en afwisselend. Ieder event wordt aan één window toegekend, en de windows overlappen elkaar niet. 

\begin{figure}
\includegraphics[width=\linewidth]{images/tumbling.png}
\caption{Tumbling event time. Learning Spark (2022)}
\end{figure}

\subsubsection{Overlapping windows}

Overlapping windows zijn time windows die elkaar onderling overlappen. Een event kan tot meerdere overlappende windows behoren.

\begin{figure}
\includegraphics[width=\linewidth]{images/overlapping.png}
\caption{Overlapping time windows. Learning Spark (2022)}
\end{figure}

\subsubsection{WordCount met time interval}

\begin{enumerate}
\item Allereerst wordt er een spark-object aangemaakt.
\item Vervolgens wordt de dataset als stream-object ingelezen. Er wordt gelezen vanuit een socket met de gewoonlijke opties. Wel wordt er een extra kolom toegevoegd, namelijk een timestamp. We veronderstellen voor de volgende stap dat het dataframe minstens twee kolommen heeft: value en timestamp.
\item 
\begin{enumerate}
\item We veronderstellen dat de value-kolom een doorlopende tekst is van alle woorden. Dit is niet nodig, dus de explode-functie wordt gebruikt om alle woorden uit de tekst te halen. Dit wordt opgeslaan in een nieuwe kolom.
\item Op basis van de timestamp en het woord wordt er gegroepeerd. Hier moet er binnen een window worden gewerkt. De window-functie wordt aangesproken op de timestamp-kolom met als tijdspanne 10 seconden en sliding interval van 5 seconden.
\end{enumerate}
\item Uiteindelijk wordt count als aggregatie-functie gebruikt. De order-by voorziet een chronologische volgorde.
\end{enumerate}

\begin{lstlisting}[language=Java]
SparkSession spark = SparkSession.builder()

Dataset<Row> lines = spark.readStream()
.format("socket")
.option("host","localhost")
.option("port",9999)
.option("includeTimestamp",true)
.load();

Dataset<Row> output = lines.withColumn("word",explode(split(col("value"),"\\s")))
.select("word","timestamp")
.groupby(window(col("timestamp"),"10 seconds", "5 seconds"), col("word"))
.count()
.orderBy(col("window"), col("count"));			

StreamingQuery query = null;

try{
query = output.writeStream()
.format("console")
.option("truncate",false)
.option("numRows", 100)
.outputMode(OutputMode.Complete())
.trigger(Trigger.ProcessingTime(5,TimeUnit.SECONDS))
.start()
} catch (TimeoutException e){
e.printStackTrace();
}

try {
query.awaitTermination();
} catch (StreamingQueryException e) {
e.printStackTrace();
}
\end{lstlisting}

\newpage

\subsection{Watermarking}

Tijdens het streamen wordt de staat van de data bijgehouden. De staat vergroot zolang het systeem data zal streamen. De kans op \textit{unbounded} data vergroot. De oplossing hiervoor is \textit{watermarking}. Een watermark is een event-time threshold. De event-time is het moment wanneer iets bij de bron werd aangemaakt Enkele kenmerken van watermarking:
\begin{itemize}
\item De watermark kijkt naar de grootste of meest recente event-time. Deze threshold definieert welke events te laat in een stream terechtkomen. Een watermark delay is de tijd dat het systeem wacht op late data, voordat het systeem de data verwerpt.
\item Enkel de relevante data wordt behouden. Het systeem verwerpt achterhaalde data.
\item Enkel de laatste timestamp wordt geüpdate. Een watermark stelt hier een bezemwagen voor. Bijvoorbeeld iedere event met een event-time voor 12u10 wordt niet meegeteld, want de range ligt tussen 12u10 en 12u20.
\end{itemize}

\begin{figure}
\includegraphics[width=\linewidth]{images/watermarking-in-windowed-grouped-aggregation.png}
\caption{De twee events komen aan tussen 12u20 en 12u25. Het watermark wordt gebruikt om het verschil tussen \textit{late} en \textit{too-late} aan te duiden. Opgehaald uit Databricks (2022)}
\end{figure}

\subsection{Streaming joins}

Er zijn twee mogelijkheden: 

\begin{enumerate}
\item Een stream joinen met een statische dataframe. 
\item Twee streams met elkaar joinen. Dit kan enkel met een outer join.
\end{enumerate}}

\subsubsection{Stream-static joins}

Data uit een gestreamede dataframe wordt gecombineerd met data uit een statische dataframe, bijvoorbeeld een dataframe opgebouwd uit een CSV. De code om dit mogelijk te maken, lijkt zeer sterk op de code voor een join bij een gewone dataframe.

\begin{itemize}
\item Dit is een stateless-operatie, daarom is er geen watermarking nodig.
\item Een stream houdt geen state van de dataframe bij.
\item Dit kan volgens een left-outer, inner of right outer join. Andere outer join opties, zoals een full outer join, zijn niet mogelijk. Het probleem is te wijten aan het incrementeel aflopen van de data. 
\item Iedere n-aantal seconden wordt de volledige statische dataframe gelezen. Caching zorgt ervoor dat de dataframe één maal moet worden gelezen. 
\item Aanpassingen aan de statische file worden tijdelijk genegeerd, vooral omdat caching wordt gebruikt. Pas bij het herstarten van de streamingquery worden de aanpassingen doorgevoerd. Dit hangt af van het gekozen bestandstype van de statische dataframe.
\end{itemize}

\begin{lstlisting}[language=Java]
Dataset<Row> staticDF = spark.read()...;
Dataset<Row> streamingDF = spark.readStream()....;
streamingDF.join(staticDF, "type");
streamingDF.join(staticDF, "type", "left_outer");
\end{lstlisting}

\subsection{Stream-Stream joins}

Data uit een stream wordt gecombineerd met data uit een andere stream. 

\begin{itemize}
\item Beide bronnen veranderen snel van inhoud. 
\item Bijvoorbeeld een stream van iedere aankoop en een stream met info van iedere klant. De twee streams kunnen met elkaar worden ge\textit{joined}.
\item De grootste uitdagingen zijn:
\begin{itemize}
\item De incompleetheid van de datastream. De kans dat er matches zijn tussen de inputs van de twee datastreams is kleiner. 
\item Events in twee streams kunnen in eender welke volgorde komen. Mogelijk met vertraging wat het moeilijker maakt om matches terug te vinden.
\end{itemize} 
\item Om deze changes aan te pakken, wordt er gewerkt met \textit{buffering}. 
\end{itemize}

\begin{lstlisting}[language=Java]
Dataset<Row> purchases = spark.readStream(). ...;
Dataset<Row> customers = spark.readStream(). ...;
Dataset<Row> matched = purchases.join(customers,”adId”);
\end{lstlisting}

\subsubsection{Buffering}

Bij buffering wordt de inputdata van beide streams tijdelijk bijgehouden. Zo wordt er gekeken naar matches, zelfs al is de data in een andere volgorde binnengekomen. Structured streaming verloopt zo effectiever en er is een beter zicht op de data.

\section{Keeping the state bounded}

Het probleem bij de stream-stream joins is dat de engine een unbounded aantal streaming state kan bijhouden. Uit het voorbeeld van de aankopen zal niet iedere klant worden gekoppeld aan een aankoop en vice versa. Rijen zullen niet worden gebruikt en zo een lange periode in de buffer blijven.

\subsubsection{State beperken}

Twee zaken moeten zijn gekend:

\begin{itemize}
\item Wat is de maximale tijdsrange tussen de twee events?
\begin{itemize}
\item In het voorbeeld van de aankopen: Hoe lang duurt het vooraleer een klant een aankoop uitvoert?
\end{itemize}
\item Wat is de maximale tijsdperiode waarin een event tussen de bron en de processing engine kan worden vertraagd?
\begin{itemize}
\item In het voorbeeld van de aankopen: Mogelijks kan er vertraging optreden bij het aankopen. De verbinding speelt hier een belangrijke rol.
\end{itemize}
\end{itemize}}

\subsubsection{State clean-up}

De \textit{delay limits} en \textit{event-time constraints} kunnen omgezet worden in dataframebewerkingen. Bij deze bewerkingen worden watermarks en \textit{time range conditions} gebruikt. Er zijn twee stappen:

\begin{enumerate}
\item Stel een watermark delay op voor beide inputstreams. Zo weet de engine hoe lang de vertraging van de input kan zijn. 
\item Stel een \textit{event-time constraint} op over de twee inputs heen. Zo weet de engine welke rijen verplicht zijn. M.a.w. de rijen die wél voldoen aan de \textit{time-constraint}. Een \textit{time-constraint} kan op twee manieren worden opgesteld:
\begin{itemize}
\item Time range join conditions zoals \textit{between righttime and (righttime + "interval 1 hour")}
\item Join op event-time windows zoals \textit{"lefttimewindow = righttimewindow"}.
\end{itemize}
\end{enumerate}

\begin{lstlisting}[language=Java]
Dataset<Row> purchases = spark.readStream()...
Dataset<Row> customers = spark.readStream()...

Dataset<Row> purchasesWWatermark = purchases.withWatermark("purchasesTime","2 hours");
Dataset<Row> customersWithWatermark = customers.withWatermark("customersTime", "3 hours");

impressionsWithwatermark.join(
clicksWithWatermark,
expr(
"custID = purchasesCustId AND " +
"customersTime >= purchasesTime AND " +
"customersTime <= purchasesTime + interval 1 hour"
)
);
\end{lstlisting}

\subsubsection{Outer joins with watermarking}

De outer-type join specifiëren gebeurt helemaal onderaan. Wel zijn er enkele opmerkingen over outer joins:
\begin{itemize}
\item De watermark delay en event-time constraints zijn verplicht. De engine moet weten wanneer een event niet gaat matchen, anders leidt dit tot een null-resultaat.
\item De \textit{outer} null-resultaten worden met een delay aangemaakt. De engine doet dit om zeker te zijn dat er mogelijke matches niet worden verworpen.
\end{itemize}

\begin{lstlisting}[language=Java]
impressionsWithWatermark.join(
	clicksWithWatermark,
	expr(
	"custID = purchasesCustId AND " +
	"customersTime >= purchasesTime AND " +
	"customersTime <= purchasesTime + interval 1 hour",
	"leftOuter"
)

);
\end{lstlisting}

\section{Voorbeelden}

\subsection{Kafka Read Example}

\begin{lstlisting}[language=Java]
public class KafkaReadExample {
	
	private static final String BOOTSTRAP_SERVERS = "localhost:9092";
	private static final String CHECKPOINT_LOCATION = "checkpoint_kafkareadexample";
	
	public static void main(String[] args)  {
		if (args.length == 0) {
			System.out.println("Usage: KafkaReadExample <topic>");
			System.exit(-1);
		}
		
		System.setProperty("hadoop.home.dir", "C:\\tmp\\winutils-extra\\hadoop");
		
		final String topic = args[0];
		
		System.out.println("Will read from the topic " + topic);
		
		SparkSession spark = SparkSession.builder()
		.appName("KafkaReadExample")				
		.master("local[*]")
		.getOrCreate();
		
		Dataset<Row> messages = spark.readStream()
		.format("kafka")
		.option("kafka.bootstrap.servers", BOOTSTRAP_SERVERS)
		.option("subscribe", topic)
		.load();
		
		System.out.println("Stream loaded");
		
		// Cast the key and the value to Strings
		messages = messages.withColumn("key", expr("CAST(key AS STRING)"))
		.withColumn("value", expr("CAST(value AS STRING)"));
		
		
		System.out.println("About to start the query");
		
		StreamingQuery query = null;
		try {
			query = messages.writeStream()
			.format("console")
			.outputMode(OutputMode.Append())
			.option("checkpointLocation", CHECKPOINT_LOCATION)
			.start();
		} catch (TimeoutException e) {		
			e.printStackTrace();
		}
		
		try {
			query.awaitTermination();
		} catch (StreamingQueryException e) {
			e.printStackTrace();
		}
		
		spark.close();
	}
}
\end{lstlisting}

\subsection{Structured Streaming Example}
\begin{lstlisting}[language=Java]
	public class StructuredStreamingExample1 {
		
		public static void main(String[] args) throws TimeoutException {
			
			System.setProperty("hadoop.home.dir", "C:\\tmp\\winutils-extra\\hadoop");
			
			// read delay from command line if specified
			int delay = args.length == 0 ? 1 : Integer.parseInt(args[0]);
			
			
			SparkSession spark = SparkSession.builder()
			.master("local[*]")
			.appName("StructuredStreamingExample1")
			.getOrCreate();
			
			// The job seems terribly slow with the default number of partitions
			spark.conf().set("spark.sql.shuffle.partitions", 4);
			
			
			Dataset<Row> lines = spark.readStream()
			.format("socket")              // read from socket
			.option("host", "localhost")   // obvious options
			.option("port", 9999)
			.load();                       // load stream into DataFrame
			
			
			// Regular transformation split line, put each word in 
			// new record (explode) 
			Dataset<Row> words = lines.select(
			explode(split(col("value"), "\\s")).as("word"));
			Dataset<Row> counts = words.groupBy("word").count();
			
			// Write the results
			DataStreamWriter<Row> writer = counts.writeStream()
			.format("console") // write to the console
			.outputMode(OutputMode.Complete()) // write everything
			.trigger(Trigger.ProcessingTime(delay, TimeUnit.SECONDS));
			
			// start execution of writer
			StreamingQuery streamingQuery = writer.start();   
			
			
			try {
				streamingQuery.awaitTermination();
			} catch (StreamingQueryException e) {			
				e.printStackTrace();
			}		
		}
		
	}
\end{lstlisting}


\newpage

\section{Oefeningen}
\subsection{Filter log messages}
\begin{lstlisting}[language=Java]
public class SparkFilterLog {
	
	private static final String SERVERS = "localhost:9092";
	private static final String CHECKPOINT_LOCATION = "checkpoint_filterlogs";
	private static final String INPUT_TOPIC = "log.messages";
	private static final String OUTPUT_TOPIC = "important.log.messages";
	private static final String HADOOP_DIR = "C:\\winutils-extra\\hadoop";
	
	public static void main(String[] args) {
		
		System.setProperty("hadoop.home.dir", HADOOP_DIR);
		
		SparkSession spark = SparkSession.builder()
			.master("local[1]")
			.appName("SparkFilterLog")
			.getOrCreate();
		
		spark.sparkContext().setLogLevel("ERROR");
		
		// Starten lezen vanuit kafka
		Dataset<Row> messages = spark.readStream()
			.format("kafka")
			.option("kafka.bootstrap.servers", SERVERS)
			.option("subscribe", INPUT_TOPIC)
			.load();
		
		// Filtering uitvoeren: enkel FATAL of ERROR
		messages = messages
			.withColumn("value",  col("value").cast(DataTypes.StringType))
			.where(col("value").like("ERROR%")
			.or(col("value").like("FATAL%")))
			.select("key","value");
		
		// Enkel de nodige fouten uitprinten naar Kafka
		StreamingQuery query = null;
		try {
			query = messages.writeStream()
				.format("kafka")
				.option("kafka.bootstrap.servers", SERVERS)
				.option("topic", OUTPUT_TOPIC)
				.option("checkpointLocation", CHECKPOINT_LOCATION)
				.outputMode(OutputMode.Append())
				.trigger(Trigger.ProcessingTime(5, TimeUnit.SECONDS))
				.start();	
		} catch (TimeoutException e) {		
			e.printStackTrace();
		}
		
		try {
			query.awaitTermination();
		} catch (StreamingQueryException e) {
			e.printStackTrace();
		}
		
		spark.close();	
	}	
}
\end{lstlisting}

\newpage

\subsection{Count log messages}
\begin{lstlisting}[language=Java]
public class SparkCountLogMessages {
	
	private static final String SERVERS = "localhost:9092";
	private static final String CHECKPOINT_LOCATION = "checkpoint_filterlogs";
	private static final String INPUT_TOPIC = "log.messages";
	private static final String OUTPUT_TOPIC = "important.log.messages";
	private static final String HADOOP_DIR = "C:\\winutils-extra\\hadoop";
	private static final String INPUT_FILE_PATH = "src/main/resources/log.messages.delayed.txt";
	
	private static final String WATERMARK_DELAY = "3 minutes";
	
	private static final String STATIC_OUTPUT = "src/main/resources/static-report";
	private static final String STREAMING_SOURCE = "src/main/resources/streaming-source";
	private static final String STREAMING_OUTPUT = "src/main/resources/streaming-report";
	
	public static void main(String[] args) throws TimeoutException, StreamingQueryException {
		
		System.setProperty("hadoop.home.dir", HADOOP_DIR);
		
		SparkSession spark = SparkSession.builder()
			.master("local[1]")
			.appName("SparkCountLog")
			.getOrCreate();
		
		spark.sparkContext().setLogLevel("ERROR");
		
		Dataset<Row> messages = spark.read().text(INPUT_FILE_PATH);
		
		// Voor static-report.
		messages = messages
			.withColumn("value", split(col("value"),"\\s"))
			.withColumn("log-level", element_at(col("value"), 1))
			.withColumn("systeem", element_at(col("value"), 2))
			.withColumn("dag", element_at(col("value"), 3))
			.withColumn("tijd", element_at(col("value"), 4))
			.withColumn("eventtime", concat(col("dag"), lit(" "), col("tijd")))
			.select("log-level", "systeem", "eventtime")
			.withColumn("window", window(col("eventtime"), "10 minutes", "5 minutes"))
			.groupBy(col("window"), col("systeem"), col("log-level"))
			.count()
			.orderBy(col("window"), desc("count"))
			.select("window.*", "systeem", "log-level", "count");
		
		
		messages.show(false);
		
		// Uitschrijven naar file.
		messages.repartition(1)
			.write()
			.mode(SaveMode.Overwrite)
			.csv(STATIC_OUTPUT);
			
		Dataset<Row> messagesStreaming = spark.readStream().text(STREAMING_SOURCE);
\end{lstlisting}

\newpage

\subsubsection{Deel II}

\begin{lstlisting}[language=Java]
		// Watermark toevoegen
		messagesStreaming = messagesStreaming
			.withColumn("value", split(col("value"),"\\s"))
			.withColumn("log-level", element_at(col("value"), 1))
			.withColumn("systeem", element_at(col("value"), 2))
			.withColumn("dag", element_at(col("value"), 3))
			.withColumn("tijd", element_at(col("value"), 4))
			.withColumn("eventtime", concat(col("dag"), lit(" "), col("tijd")))
			.select("log-level", "systeem", "eventtime")
			// String-types werken niet voor watermarks, dus casten.
			.withColumn("eventtime", col("eventtime").cast(DataTypes.TimestampType))
			.withWatermark("eventtime",  WATERMARK_DELAY)
			.withColumn("window", window(col("eventtime"), "10 minutes", "5 minutes"))
			.groupBy(col("window"), col("systeem"), col("log-level"))
			.count()
		// Sorteren kan enkel bij complete output!
		//.orderBy(col("window"), desc("count"))
			.select("window.*", "systeem", "log-level", "count");
		
		
		
		// Watermark is verplicht!!
		StreamingQuery query = messagesStreaming
			.writeStream()
			.format("csv")
			.option("path", STREAMING_OUTPUT)
			.outputMode(OutputMode.Append())
			.option("checkpointLocation", CHECKPOINT_LOCATION)
			.trigger(Trigger.ProcessingTime(5, TimeUnit.SECONDS))
			.start();
		
		query.awaitTermination();
		spark.close();	
	}	
}
\end{lstlisting}

\end{document}